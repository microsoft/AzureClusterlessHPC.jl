{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AzureClusterlessHPC.jl - Simplified parallel computing on Azure with Julia Overview AzureClusterlessHPC.jl is a package for simplified batch computing in the cloud. AzureClusterlessHPC.jl borrows the syntax of Julia's Distributed Programming package to easily execute parallel Julia workloads in the cloud using batch computing services such as Azure Batch . Troubleshooting Contact the developer at pwitte@microsoft.com .","title":"Home"},{"location":"#azureclusterlesshpcjl-simplified-parallel-computing-on-azure-with-julia","text":"","title":"AzureClusterlessHPC.jl - Simplified parallel computing on Azure with Julia"},{"location":"#overview","text":"AzureClusterlessHPC.jl is a package for simplified batch computing in the cloud. AzureClusterlessHPC.jl borrows the syntax of Julia's Distributed Programming package to easily execute parallel Julia workloads in the cloud using batch computing services such as Azure Batch .","title":"Overview"},{"location":"#troubleshooting","text":"Contact the developer at pwitte@microsoft.com .","title":"Troubleshooting"},{"location":"about/","text":"About AzureClusterlessHPC.jl is developed and maintained by the Microsoft Research for Industry (RFI) team. Copyright Copyright (c) Microsoft Corporation. All rights reserved. Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.","title":"About"},{"location":"about/#about","text":"AzureClusterlessHPC.jl is developed and maintained by the Microsoft Research for Industry (RFI) team.","title":"About"},{"location":"about/#copyright","text":"Copyright (c) Microsoft Corporation. All rights reserved. Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.","title":"Copyright"},{"location":"broadcasting/","text":"Broadcasting Broadcast an expression to all batch workers of (future) batch jobs and return a batch future. The batch future can be passed as a function argument instead of the variable. batch_future = @bcast expr The use of @bcast is recommended to pass large arguments to functions (e.g. arrays). This avoids copying input arguments to each individual task separately. Instead, expressions tagged via @batchdef are uploaded to blob storage once and their blob reference is passed to one or multiple tasks. To access a broadcasted variable inside an executed function, use the fetch or fetch! (in-place) function: # Create and broadcast array A = randn(2, 2) _A = @bcast A # Define function @batchdef function print_array(_A) A = fetch(_A) # load A into memory print(A) end # Remotely execute function @batchexec print_array(_A) # pass batch future Calling A = fetch(_A) on the local machine (rather than on a batch worker) downloads the broadcasted variable from blob storage and returns it.","title":"Broadcasting"},{"location":"broadcasting/#broadcasting","text":"Broadcast an expression to all batch workers of (future) batch jobs and return a batch future. The batch future can be passed as a function argument instead of the variable. batch_future = @bcast expr The use of @bcast is recommended to pass large arguments to functions (e.g. arrays). This avoids copying input arguments to each individual task separately. Instead, expressions tagged via @batchdef are uploaded to blob storage once and their blob reference is passed to one or multiple tasks. To access a broadcasted variable inside an executed function, use the fetch or fetch! (in-place) function: # Create and broadcast array A = randn(2, 2) _A = @bcast A # Define function @batchdef function print_array(_A) A = fetch(_A) # load A into memory print(A) end # Remotely execute function @batchexec print_array(_A) # pass batch future Calling A = fetch(_A) on the local machine (rather than on a batch worker) downloads the broadcasted variable from blob storage and returns it.","title":"Broadcasting"},{"location":"cleanup/","text":"Clean up resources After executing a batch job via @batchexec , you can use the returned batch controller to clean up resources: # Batch job batch_controller = @batchexec print(\"Hello world\") # Terminate job terminate_job(batch_controller) # Delete job delete_job(batch_controller) # Delete pool delete_pool(batch_controller) # Delete blob container with all temporary files delte_container(batch_controller) # Or alternatively, delete pool + job + container together destroy!(batch_controller) If you did not return a batch controller, you can call the following functions without any input arguments, in which case they will delete the pool and container as specified in your parameter.json file (or the default ones). The delete_all_jobs function will delete all exisiting jobs that start with the job id specified in the parameter file. # Delete container delete_container() # Delete pool delete_pool() # Delete all jobs delete_all_jobs()","title":"Clean up"},{"location":"cleanup/#clean-up-resources","text":"After executing a batch job via @batchexec , you can use the returned batch controller to clean up resources: # Batch job batch_controller = @batchexec print(\"Hello world\") # Terminate job terminate_job(batch_controller) # Delete job delete_job(batch_controller) # Delete pool delete_pool(batch_controller) # Delete blob container with all temporary files delte_container(batch_controller) # Or alternatively, delete pool + job + container together destroy!(batch_controller) If you did not return a batch controller, you can call the following functions without any input arguments, in which case they will delete the pool and container as specified in your parameter.json file (or the default ones). The delete_all_jobs function will delete all exisiting jobs that start with the job id specified in the parameter file. # Delete container delete_container() # Delete pool delete_pool() # Delete all jobs delete_all_jobs()","title":"Clean up resources"},{"location":"credentials/","text":"Parameters and credentials Credentials (one batch and storage account) To use a single Azure Batch and storage account, you can set up a single combined credential file for both accounts. The required information must provided as a JSON file containing user credentials for Azure blob storage and Azure batch. Azure Batch requires authentication via the Azure Active Directory (AAD), whereas the blob storage account must be authenticated with a secret key. Refer to the Azure documentation for information on how to authenticate Azure Batch via the AAD. Use the following template to create a file called credentials.json file and fill in your keys and ids. Safely store this file and never upload it to public repositories: { \"_AD_TENANT\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"_AD_BATCH_CLIENT_ID\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"_AD_SECRET_BATCH\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\", \"_BATCH_ACCOUNT_URL\": \"https://batchaccountname.batchregion.batch.azure.com\", \"_BATCH_RESOURCE\": \"https://batch.core.windows.net/\", \"_STORAGE_ACCOUNT_NAME\": \"storageaccountname\", \"_STORAGE_ACCOUNT_KEY\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" } When using AzureClusterlessHPC, set the environment variable ENV[\"CREDENTIALS\"] = \"/path/to/credentials.json\" before you load the package via using AzureClusterlessHPC . Credentials (multiple batch and storage accounts) AzureClusterlessHPC also allows using multiple storage and/or batch accounts. Using multiple batch accounts provides the possiblity to cirumvent service limits of a single batch account or it allows to distribute workloads among multiple regions. If you create batch accounts for multiple regions, you need to have at least one storage account in each region. To automatically create multiple batch and storage accounts, use the shell script create_azure_accounts.sh . Pass the list of region(s) and the number of accounts per region as command line arguments to the script. E.g., to create two batch and storeage acounts in each US West and South Central US (i.e, total of 4 batch and 4 storage accounts), run: # Go to AzureClusterlessHPC directory cd /path/to/AzureClusterlessHPC # Azure CLI log in az login # Create accounts ./create_azure_accounts \"westus southcentralus\" 2 Creating the accounts may take several minutes, depending on how many accounts are being created. The script also fetches the required credentials and stores them in the directory user_data . No further actions from the user side are required. To use the credentials stored in user_data with AzureClusterlessHPC, make sure that the environment variable \"CREDENTIALS\" is unset (run unset CREDENTIALS from the bash command line). If CREDENTIALS is not set, AzureClusterlessHPC will automatically look for credentials in user_data . After loading AzureClusterlessHPC in Julia ( using AzureClusterlessHPC ), you can check which accounts were found by checking AzureClusterlessHPC.__credentials__ . This returns a list with one entry per available batch account. Type AzureClusterlessHPC.__credentials__[i] to print the credential information for the i-th account. Batch parameters Users can optionally provide a parameters.json file that specifies pool and job parameters. Set the environment variable ENV[\"PARAMETERS\"]=/path/to/parameters.json before loading the package (see section \"Quickstart\" for an example). The following set of parameters and default values are used, unless specified otherwise by the user: { \"_POOL_ID\": \"BatchPool\", \"_POOL_COUNT\": \"1\", \"_NODE_COUNT_PER_POOL\": \"1\", \"_POOL_VM_SIZE\": \"Standard_E2s_v3\", \"_JOB_ID\": \"BatchJob\", \"_STANDARD_OUT_FILE_NAME\": \"stdout.txt\", \"_NODE_OS_PUBLISHER\": \"Canonical\", \"_NODE_OS_OFFER\": \"UbuntuServer\", \"_NODE_OS_SKU\": \"18.04\", \"_BLOB_CONTAINER\": \"redwoodtemp\", \"_INTER_NODE_CONNECTION\": \"0\", \"_NUM_RETRYS\": \"0\", \"_MPI_RUN\": \"0\", \"_CONTAINER\": \"None\", \"_NUM_NODES_PER_TASK\": \"1\", \"_NUM_PROCS_PER_NODE\": \"1\", \"_OMP_NUM_THREADS\": \"1\", \"_JULIA_DEPOT_PATH\": \"/mnt/batch/tasks/startup/wd/.julia\", \"_PYTHONPATH\": \"/mnt/batch/tasks/startup/wd/.local/lib/python3.6/site-packages\" } Note: Do not modify the \"_JULIA_DEPOT_PATH\" and \"_PYTHONPATH\" unless you use a pool with a custom image in which Julia has been already installed. In that case, set the depot path to the location of the .julia directory.","title":"Credentials"},{"location":"credentials/#parameters-and-credentials","text":"","title":"Parameters and credentials"},{"location":"credentials/#credentials-one-batch-and-storage-account","text":"To use a single Azure Batch and storage account, you can set up a single combined credential file for both accounts. The required information must provided as a JSON file containing user credentials for Azure blob storage and Azure batch. Azure Batch requires authentication via the Azure Active Directory (AAD), whereas the blob storage account must be authenticated with a secret key. Refer to the Azure documentation for information on how to authenticate Azure Batch via the AAD. Use the following template to create a file called credentials.json file and fill in your keys and ids. Safely store this file and never upload it to public repositories: { \"_AD_TENANT\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"_AD_BATCH_CLIENT_ID\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"_AD_SECRET_BATCH\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\", \"_BATCH_ACCOUNT_URL\": \"https://batchaccountname.batchregion.batch.azure.com\", \"_BATCH_RESOURCE\": \"https://batch.core.windows.net/\", \"_STORAGE_ACCOUNT_NAME\": \"storageaccountname\", \"_STORAGE_ACCOUNT_KEY\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" } When using AzureClusterlessHPC, set the environment variable ENV[\"CREDENTIALS\"] = \"/path/to/credentials.json\" before you load the package via using AzureClusterlessHPC .","title":"Credentials (one batch and storage account)"},{"location":"credentials/#credentials-multiple-batch-and-storage-accounts","text":"AzureClusterlessHPC also allows using multiple storage and/or batch accounts. Using multiple batch accounts provides the possiblity to cirumvent service limits of a single batch account or it allows to distribute workloads among multiple regions. If you create batch accounts for multiple regions, you need to have at least one storage account in each region. To automatically create multiple batch and storage accounts, use the shell script create_azure_accounts.sh . Pass the list of region(s) and the number of accounts per region as command line arguments to the script. E.g., to create two batch and storeage acounts in each US West and South Central US (i.e, total of 4 batch and 4 storage accounts), run: # Go to AzureClusterlessHPC directory cd /path/to/AzureClusterlessHPC # Azure CLI log in az login # Create accounts ./create_azure_accounts \"westus southcentralus\" 2 Creating the accounts may take several minutes, depending on how many accounts are being created. The script also fetches the required credentials and stores them in the directory user_data . No further actions from the user side are required. To use the credentials stored in user_data with AzureClusterlessHPC, make sure that the environment variable \"CREDENTIALS\" is unset (run unset CREDENTIALS from the bash command line). If CREDENTIALS is not set, AzureClusterlessHPC will automatically look for credentials in user_data . After loading AzureClusterlessHPC in Julia ( using AzureClusterlessHPC ), you can check which accounts were found by checking AzureClusterlessHPC.__credentials__ . This returns a list with one entry per available batch account. Type AzureClusterlessHPC.__credentials__[i] to print the credential information for the i-th account.","title":"Credentials (multiple batch and storage accounts)"},{"location":"credentials/#batch-parameters","text":"Users can optionally provide a parameters.json file that specifies pool and job parameters. Set the environment variable ENV[\"PARAMETERS\"]=/path/to/parameters.json before loading the package (see section \"Quickstart\" for an example). The following set of parameters and default values are used, unless specified otherwise by the user: { \"_POOL_ID\": \"BatchPool\", \"_POOL_COUNT\": \"1\", \"_NODE_COUNT_PER_POOL\": \"1\", \"_POOL_VM_SIZE\": \"Standard_E2s_v3\", \"_JOB_ID\": \"BatchJob\", \"_STANDARD_OUT_FILE_NAME\": \"stdout.txt\", \"_NODE_OS_PUBLISHER\": \"Canonical\", \"_NODE_OS_OFFER\": \"UbuntuServer\", \"_NODE_OS_SKU\": \"18.04\", \"_BLOB_CONTAINER\": \"redwoodtemp\", \"_INTER_NODE_CONNECTION\": \"0\", \"_NUM_RETRYS\": \"0\", \"_MPI_RUN\": \"0\", \"_CONTAINER\": \"None\", \"_NUM_NODES_PER_TASK\": \"1\", \"_NUM_PROCS_PER_NODE\": \"1\", \"_OMP_NUM_THREADS\": \"1\", \"_JULIA_DEPOT_PATH\": \"/mnt/batch/tasks/startup/wd/.julia\", \"_PYTHONPATH\": \"/mnt/batch/tasks/startup/wd/.local/lib/python3.6/site-packages\" } Note: Do not modify the \"_JULIA_DEPOT_PATH\" and \"_PYTHONPATH\" unless you use a pool with a custom image in which Julia has been already installed. In that case, set the depot path to the location of the .julia directory.","title":"Batch parameters"},{"location":"faq/","text":"FAQ How does AzureClusterlessHPC work? Whenever you tag an expression with @batchdef , AzureClusterlessHPC collects the abstract syntax tree (AST) of the expressions and appends it to a global collection. You can print the currently collected AST via batch_show() and you can reset the collected expressions via batch_clear() . When you use @batchexec , AzureClusterlessHPC creates a closure around the executed expression and uploads it, along with the collected AST as a batch resource file. AzureClusterlessHPC also anayzes the executed funtion and replaces return statements with serializations, so that return arguments are written to the local disk of the batch worker and subsequently uploaded to blob storage, from where they can be collected via the fetch / fetch! functions. What costs does AzureClusterlessHPC incur? AzureClusterlessHPC calls Azure Batch and Azure Blob Storage APIs. Costs incur for operations that write data to blob storage, download or store it (e.g. @bcast , @batchexec , fetch , fetch! ). For batch jobs, costs incur for the requested VMs in the batch pool (regardless of whether jobs are currently running or not). How do I clean up and shut down all services that invoke costs? Costs are invoked by a batch pool made up of one or multiple VMs and by files stored in blob storage. To shut down the pool run delete_pool and to delete the blob container that contains any temporary files run delete_container() . These actions will delete the pool and blob container specified in your parameter JSON file (or the default ones created by AzureClusterlessHPC). How can I specify Julia packages to be installed on the batch worker nodes? To specify Julia packages that are installed on the worker nodes, create a pool startup script and use the create_pool_and_resource_file function to launch the pool. Refer to the section \"Create a batch pool\" for details. How can I start a pool with a custom VM image? To start a pool with a custom VM image, you need to first create a custom VM image and then upload it to the Azure shared image gallery. The image gallery will assign an image reference ID to the image (see here for details on how to create a shared image). When starting your batch pool, pass this ID to the pool startup function: create_pool(image_resource_id=\"shared_image_id\") . What kind of input and return arguments are supported in functions executed via @batchexec ? AzureClusterlessHPC.jl supports any kind of input and return arguments, including custom data structures. Input and return arguments do not need to be JSON serializable. However, we recommend using the same Julia version on the batch workers as on your local machine or master VM. This avoids possible inconsistencies when serializing/deserializing arguments and expressions. Are MPI and multi-node batch tasks supported? Yes, you can execute AzureClusterlessHPC tasks via Julia MPI on either single VMs or on multiple VMs. See the above section MPI support for details on how to runs batch tasks with MPI support.","title":"FAQ"},{"location":"faq/#faq","text":"How does AzureClusterlessHPC work? Whenever you tag an expression with @batchdef , AzureClusterlessHPC collects the abstract syntax tree (AST) of the expressions and appends it to a global collection. You can print the currently collected AST via batch_show() and you can reset the collected expressions via batch_clear() . When you use @batchexec , AzureClusterlessHPC creates a closure around the executed expression and uploads it, along with the collected AST as a batch resource file. AzureClusterlessHPC also anayzes the executed funtion and replaces return statements with serializations, so that return arguments are written to the local disk of the batch worker and subsequently uploaded to blob storage, from where they can be collected via the fetch / fetch! functions. What costs does AzureClusterlessHPC incur? AzureClusterlessHPC calls Azure Batch and Azure Blob Storage APIs. Costs incur for operations that write data to blob storage, download or store it (e.g. @bcast , @batchexec , fetch , fetch! ). For batch jobs, costs incur for the requested VMs in the batch pool (regardless of whether jobs are currently running or not). How do I clean up and shut down all services that invoke costs? Costs are invoked by a batch pool made up of one or multiple VMs and by files stored in blob storage. To shut down the pool run delete_pool and to delete the blob container that contains any temporary files run delete_container() . These actions will delete the pool and blob container specified in your parameter JSON file (or the default ones created by AzureClusterlessHPC). How can I specify Julia packages to be installed on the batch worker nodes? To specify Julia packages that are installed on the worker nodes, create a pool startup script and use the create_pool_and_resource_file function to launch the pool. Refer to the section \"Create a batch pool\" for details. How can I start a pool with a custom VM image? To start a pool with a custom VM image, you need to first create a custom VM image and then upload it to the Azure shared image gallery. The image gallery will assign an image reference ID to the image (see here for details on how to create a shared image). When starting your batch pool, pass this ID to the pool startup function: create_pool(image_resource_id=\"shared_image_id\") . What kind of input and return arguments are supported in functions executed via @batchexec ? AzureClusterlessHPC.jl supports any kind of input and return arguments, including custom data structures. Input and return arguments do not need to be JSON serializable. However, we recommend using the same Julia version on the batch workers as on your local machine or master VM. This avoids possible inconsistencies when serializing/deserializing arguments and expressions. Are MPI and multi-node batch tasks supported? Yes, you can execute AzureClusterlessHPC tasks via Julia MPI on either single VMs or on multiple VMs. See the above section MPI support for details on how to runs batch tasks with MPI support.","title":"FAQ"},{"location":"functioncalls/","text":"Remote function calls via batch \\@batchdef Execute an expression under Main and on the batch workers of a (future) batch job that is executed from the same Julia session (equivalent to @everywhere for parallel Julia sessions). @batchdef expr @batchdef can be used to define variables, functions or with include and using statements: # Import packages @batchdef using LinearAlgebra, Random # Includes @batchdef include(\"testfile.jl\") # Define variables @batchdef A = ones(2, 2) # Define functions @batchdef hello_world(name) = print(\"Hello $name\") You can define multiple expression with @batchdef using a begin ... end block: @batchdef begin A = ones(1, 1) B = zeros(1, 1) end Expressions that are tagged via @batchdef are collected by AzureClusterlessHPC and are used in subsequent batch job executions. To print the current collection of expressions, type batch_show() . To reset the batch environment and remove all prior expressions from the call stack, use batch_clear() (or restart the Julia session). \\@batchexec Execute an expression as a batch job (equivalent to @spawn for parallel Julia sessions). @batchexec expr The primary purpose of @batchexec is to execute functions that have been priorly defined with @batchdef . E.g. # Define function @batchdef function hello_world(name) print(\"Hello $name\") return \"Goodbye\" end # Call function via batch bctrl = @batchexec hello_world(\"Bob\") Arguments for functions executed via @batchexec are always passed by copy . This is important to keep in mind when passing large arguments to a function that is executed as a multi-task batch job, in which case arguments are copied to each task separately. To pass large arguments to a multi-task batch job, use the @bcast macro (see next section). To execute a multi-task batch job, use the pmap function: # Multi-task batch job bctrl = @batchexec pmap(name -> hello_world(name), [\"Bob\", \"Jane\"]) The @batchexec macro returns a batch controller ( bctrl ) that can be used for the following actions: Wait for all tasks of the batch job to finish: wait_for_tasks_to_complete(bctrl) Terminate the batch job: terminate_job(bctrl) Delete the batch job: delete_job(bctrl) Delete the pool: delete_pool(bctrl) Delete the blob container in which all temporary files are stored: delete_container(bctrl) Destroy all Azure resources associated with the batch controller (job, pool, container): destroy!(bctrl) Fetch the output of all tasks: output = fetch(bctrl) . This operation is blocking and waits for all tasks to finish. The output is collected asynchonously in order of completion. Fetch the output of task i : output = fetch(bctrl, i) (blocking for that task). Inplace fetch (all tasks). Returns output and overwrites the blob future in bctrl.output : output = fetch!(bctrl) (blocking operation) Inplace fetch (task i ): output = fetch!(bctrl, i) (blocking for task i ) Fetch output of all tasks and apply a reduction operation to the output (along tasks): output_reduce = fetchreduce(bctrl; op=+) (blocking) Inplace fetch and reduce (overwrite output_reduce ): fetchreduce!(bctrl, output_reduce; op=+) (blocking) Limitations: Function return arguments must be explicitley returned via the return statement. I.e., implicit returns in which the final function expression is automatically returned are not supported. Functions executed via @batchexec can only have a single return argument. I.e. control structures such as if ... else ... end with multiple return statements are not supported and will throw an exception when fetching the output. Function arguments are passed by copy, never by reference. MPI support You can execute tasks via Julia MPI on either single VMs or on multiple VMs. To enable MPI on a single VM (shared memory parallelism), set the following variables in your parameters.json file: \"_INTER_NODE_CONNECTION\": \"0\", \"_MPI_RUN\": \"1\", \"_NUM_NODES_PER_TASK\": \"1\", \"_NUM_PROCS_PER_NODE\": \"2\", \"_OMP_NUM_THREADS\": \"1\" Note, that \"_NUM_NODES_PER_TASK\" must be set to 1 if \"_INTER_NODE_CONNECTION\" is set to \"0\" . \"_NUM_PROCS_PER_NODE\" specifies the number of MPI ranks per node and \"_OMP_NUM_THREADS\" specifies the number of OpenMP threads per rank (if applicable). To enable MPI tasks on multiple instances (distributed memory parallelism), set: \"_INTER_NODE_CONNECTION\": \"1\", \"_MPI_RUN\": \"1\", \"_NUM_NODES_PER_TASK\": \"2\", \"_NUM_PROCS_PER_NODE\": \"4\", \"_OMP_NUM_THREADS\": \"1\" The total number of MPI ranks for each task is given by \"_NUM_NODES_PER_TASK\" times \"_NUM_PROCS_PER_NODE\" . E.g. in this example, each MPI task is executed on 2 nodes with 4 processes per node, i.e. 8 MPI ranks in total. In your application, you need to load the Julia MPI package via @batchdef . For a full MPI example, see AzureClusterlessHPC/examples/mpi/julia_batch_mpi.ipynb .","title":"Remote function calls"},{"location":"functioncalls/#remote-function-calls-via-batch","text":"","title":"Remote function calls via batch"},{"location":"functioncalls/#batchdef","text":"Execute an expression under Main and on the batch workers of a (future) batch job that is executed from the same Julia session (equivalent to @everywhere for parallel Julia sessions). @batchdef expr @batchdef can be used to define variables, functions or with include and using statements: # Import packages @batchdef using LinearAlgebra, Random # Includes @batchdef include(\"testfile.jl\") # Define variables @batchdef A = ones(2, 2) # Define functions @batchdef hello_world(name) = print(\"Hello $name\") You can define multiple expression with @batchdef using a begin ... end block: @batchdef begin A = ones(1, 1) B = zeros(1, 1) end Expressions that are tagged via @batchdef are collected by AzureClusterlessHPC and are used in subsequent batch job executions. To print the current collection of expressions, type batch_show() . To reset the batch environment and remove all prior expressions from the call stack, use batch_clear() (or restart the Julia session).","title":"\\@batchdef"},{"location":"functioncalls/#batchexec","text":"Execute an expression as a batch job (equivalent to @spawn for parallel Julia sessions). @batchexec expr The primary purpose of @batchexec is to execute functions that have been priorly defined with @batchdef . E.g. # Define function @batchdef function hello_world(name) print(\"Hello $name\") return \"Goodbye\" end # Call function via batch bctrl = @batchexec hello_world(\"Bob\") Arguments for functions executed via @batchexec are always passed by copy . This is important to keep in mind when passing large arguments to a function that is executed as a multi-task batch job, in which case arguments are copied to each task separately. To pass large arguments to a multi-task batch job, use the @bcast macro (see next section). To execute a multi-task batch job, use the pmap function: # Multi-task batch job bctrl = @batchexec pmap(name -> hello_world(name), [\"Bob\", \"Jane\"]) The @batchexec macro returns a batch controller ( bctrl ) that can be used for the following actions: Wait for all tasks of the batch job to finish: wait_for_tasks_to_complete(bctrl) Terminate the batch job: terminate_job(bctrl) Delete the batch job: delete_job(bctrl) Delete the pool: delete_pool(bctrl) Delete the blob container in which all temporary files are stored: delete_container(bctrl) Destroy all Azure resources associated with the batch controller (job, pool, container): destroy!(bctrl) Fetch the output of all tasks: output = fetch(bctrl) . This operation is blocking and waits for all tasks to finish. The output is collected asynchonously in order of completion. Fetch the output of task i : output = fetch(bctrl, i) (blocking for that task). Inplace fetch (all tasks). Returns output and overwrites the blob future in bctrl.output : output = fetch!(bctrl) (blocking operation) Inplace fetch (task i ): output = fetch!(bctrl, i) (blocking for task i ) Fetch output of all tasks and apply a reduction operation to the output (along tasks): output_reduce = fetchreduce(bctrl; op=+) (blocking) Inplace fetch and reduce (overwrite output_reduce ): fetchreduce!(bctrl, output_reduce; op=+) (blocking) Limitations: Function return arguments must be explicitley returned via the return statement. I.e., implicit returns in which the final function expression is automatically returned are not supported. Functions executed via @batchexec can only have a single return argument. I.e. control structures such as if ... else ... end with multiple return statements are not supported and will throw an exception when fetching the output. Function arguments are passed by copy, never by reference.","title":"\\@batchexec"},{"location":"functioncalls/#mpi-support","text":"You can execute tasks via Julia MPI on either single VMs or on multiple VMs. To enable MPI on a single VM (shared memory parallelism), set the following variables in your parameters.json file: \"_INTER_NODE_CONNECTION\": \"0\", \"_MPI_RUN\": \"1\", \"_NUM_NODES_PER_TASK\": \"1\", \"_NUM_PROCS_PER_NODE\": \"2\", \"_OMP_NUM_THREADS\": \"1\" Note, that \"_NUM_NODES_PER_TASK\" must be set to 1 if \"_INTER_NODE_CONNECTION\" is set to \"0\" . \"_NUM_PROCS_PER_NODE\" specifies the number of MPI ranks per node and \"_OMP_NUM_THREADS\" specifies the number of OpenMP threads per rank (if applicable). To enable MPI tasks on multiple instances (distributed memory parallelism), set: \"_INTER_NODE_CONNECTION\": \"1\", \"_MPI_RUN\": \"1\", \"_NUM_NODES_PER_TASK\": \"2\", \"_NUM_PROCS_PER_NODE\": \"4\", \"_OMP_NUM_THREADS\": \"1\" The total number of MPI ranks for each task is given by \"_NUM_NODES_PER_TASK\" times \"_NUM_PROCS_PER_NODE\" . E.g. in this example, each MPI task is executed on 2 nodes with 4 processes per node, i.e. 8 MPI ranks in total. In your application, you need to load the Julia MPI package via @batchdef . For a full MPI example, see AzureClusterlessHPC/examples/mpi/julia_batch_mpi.ipynb .","title":"MPI support"},{"location":"installation/","text":"Installation and prerequisites To install AzureClusterlessHPC.jl, run the following command from an interactive Julia session (press the ] key and then type the command). When prompted, enter the user name and password that were provided to you: ] add https://github.com/microsoft/AzureClusterlessHPC.jl AzureClusterlessHPC requires the Azure software development kits (SDKs) for batch computing, blob storage and common functionalities. See pyrequirements.txt for the full list of current requirements. To install the required packages, run # Go to AzureClusterlessHPC directory cd /path/to/AzureClusterlessHPC pip3 install -r pyrequirements.txt","title":"Installation"},{"location":"installation/#installation-and-prerequisites","text":"To install AzureClusterlessHPC.jl, run the following command from an interactive Julia session (press the ] key and then type the command). When prompted, enter the user name and password that were provided to you: ] add https://github.com/microsoft/AzureClusterlessHPC.jl AzureClusterlessHPC requires the Azure software development kits (SDKs) for batch computing, blob storage and common functionalities. See pyrequirements.txt for the full list of current requirements. To install the required packages, run # Go to AzureClusterlessHPC directory cd /path/to/AzureClusterlessHPC pip3 install -r pyrequirements.txt","title":"Installation and prerequisites"},{"location":"output/","text":"Collect output Fetch output Executing a function as a batch job via @batchexec returns a batch controller of type BatchController : # Test function @batchdef function hello_world(n) A = zeros(n, n) B = ones(n, n) return A, B end # Execute function as a multi-task batch job n = 2 batch_controller = @batchexec pmap(() -> hello_world(n), 1:2) # 2 tasks The batch controller has a field called batch_controller.output , which is a cell array of blob futures. The blob futures contain a (randomly generated) blob name of the future result stored in blob storage. E.g.: julia> batch_controller.output 2-element Array{Any,1}: BlobFuture(\"redwoodtemp\", BlobRef((\"o9UspZStMmqn\", \"TwIMfLrYiac2\"))) BlobFuture(\"redwoodtemp\", BlobRef((\"PxgtEgZonWPJ\", \"kZz1Wuknnag0\"))) The cell array contains one entry per task, i.e. length(batch_controller.output) is equal to the number of tasks of the executed batch job (in this case 2). As our function returns two arguments, each BlobRef contains two (future) blob names. To fetch the output of an executed function, AzureClusterlessHPC provides the fetch and fetch! functions. These functions can be either called on the batch controller output = fetch(batch_controller) or they can be directly called on the blob futures: # fetch called on batch controller output_job = fetch(batch_controller) # fetch called on blob future output_task_1 = fetch(batch_controller.output[1]) However, we recommend to always call fetch on the batch controller and not on the batch futures in .output . Calling fetch(batch_controller) is a blocking operation and waits for all batch tasks to terminate. Calling fetch(batch_controller.output[1]) is non-blocking and throws an exception if the task or job has not yet finished and the output is not yet available in blob storage. AzureClusterlessHPC also supplies in-place fetch functions, which not only return the output, but they also overwrite the BlobRef of the BlobFuture in batch_controller.output : # Inplace fetch output = fetch!(batch_controller) 2-element Array{Any,1}: ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0]) ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0]) batch_controller.output 2-element Array{Any,1}: BlobFuture(\"redwoodtemp\", ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0])) BlobFuture(\"redwoodtemp\", ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0])) Inplace fetch! by default deletes the referenced blob objects. If fetch! is called on the batch controller again, it will then throw an error. To avoid deleting the blob, call fetch!(batch_controller; destroy_blob=false) . Fetch output and apply reduction operation AzureClusterlessHPC supplies the fetchreduce and fetchreduce! functions to collect the output from multiple tasks and apply a specified reduction operation to the output. E.g. using the prior example: # Test function @batchdef function hello_world(n) A = ones(n, n) B = 2 .* ones(n, n) return A, B end # Execute function as a multi-task batch job n = 2 batch_controller = @batchexec pmap(() -> hello_world(n), 1:2) # 2 tasks We can fetch and sum the output via: output_sum = fetchreduce(batch_controller; op=+, remote=false) # Returns ([2.0 2.0; 2.0 2.0], [4.0 4.0; 4.0 4.0]) The remote keyword argument specifies where the summation is execute. By default, the output is collected and summed on the master. For remote=true , AzureClusterlessHPC will schedule the summation tasks on idle instances in the batch pool and only the final (reduced) argument is copied back to the master. We can also initialize the output ourselves and then call the in-place fetchreduce! function: # Initialize output output = (zeros(2, 2), zeros(2, 2)) # Fetch output and sum fetchreduce!(batch_controller, output; op=+) @show output output = ([2.0 2.0; 2.0 2.0], [4.0 4.0; 4.0 4.0])","title":"Collect output"},{"location":"output/#collect-output","text":"","title":"Collect output"},{"location":"output/#fetch-output","text":"Executing a function as a batch job via @batchexec returns a batch controller of type BatchController : # Test function @batchdef function hello_world(n) A = zeros(n, n) B = ones(n, n) return A, B end # Execute function as a multi-task batch job n = 2 batch_controller = @batchexec pmap(() -> hello_world(n), 1:2) # 2 tasks The batch controller has a field called batch_controller.output , which is a cell array of blob futures. The blob futures contain a (randomly generated) blob name of the future result stored in blob storage. E.g.: julia> batch_controller.output 2-element Array{Any,1}: BlobFuture(\"redwoodtemp\", BlobRef((\"o9UspZStMmqn\", \"TwIMfLrYiac2\"))) BlobFuture(\"redwoodtemp\", BlobRef((\"PxgtEgZonWPJ\", \"kZz1Wuknnag0\"))) The cell array contains one entry per task, i.e. length(batch_controller.output) is equal to the number of tasks of the executed batch job (in this case 2). As our function returns two arguments, each BlobRef contains two (future) blob names. To fetch the output of an executed function, AzureClusterlessHPC provides the fetch and fetch! functions. These functions can be either called on the batch controller output = fetch(batch_controller) or they can be directly called on the blob futures: # fetch called on batch controller output_job = fetch(batch_controller) # fetch called on blob future output_task_1 = fetch(batch_controller.output[1]) However, we recommend to always call fetch on the batch controller and not on the batch futures in .output . Calling fetch(batch_controller) is a blocking operation and waits for all batch tasks to terminate. Calling fetch(batch_controller.output[1]) is non-blocking and throws an exception if the task or job has not yet finished and the output is not yet available in blob storage. AzureClusterlessHPC also supplies in-place fetch functions, which not only return the output, but they also overwrite the BlobRef of the BlobFuture in batch_controller.output : # Inplace fetch output = fetch!(batch_controller) 2-element Array{Any,1}: ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0]) ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0]) batch_controller.output 2-element Array{Any,1}: BlobFuture(\"redwoodtemp\", ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0])) BlobFuture(\"redwoodtemp\", ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0])) Inplace fetch! by default deletes the referenced blob objects. If fetch! is called on the batch controller again, it will then throw an error. To avoid deleting the blob, call fetch!(batch_controller; destroy_blob=false) .","title":"Fetch output"},{"location":"output/#fetch-output-and-apply-reduction-operation","text":"AzureClusterlessHPC supplies the fetchreduce and fetchreduce! functions to collect the output from multiple tasks and apply a specified reduction operation to the output. E.g. using the prior example: # Test function @batchdef function hello_world(n) A = ones(n, n) B = 2 .* ones(n, n) return A, B end # Execute function as a multi-task batch job n = 2 batch_controller = @batchexec pmap(() -> hello_world(n), 1:2) # 2 tasks We can fetch and sum the output via: output_sum = fetchreduce(batch_controller; op=+, remote=false) # Returns ([2.0 2.0; 2.0 2.0], [4.0 4.0; 4.0 4.0]) The remote keyword argument specifies where the summation is execute. By default, the output is collected and summed on the master. For remote=true , AzureClusterlessHPC will schedule the summation tasks on idle instances in the batch pool and only the final (reduced) argument is copied back to the master. We can also initialize the output ourselves and then call the in-place fetchreduce! function: # Initialize output output = (zeros(2, 2), zeros(2, 2)) # Fetch output and sum fetchreduce!(batch_controller, output; op=+) @show output output = ([2.0 2.0; 2.0 2.0], [4.0 4.0; 4.0 4.0])","title":"Fetch output and apply reduction operation"},{"location":"pool/","text":"Set up a batch pool Start a pool and optionally install Julia packages on the workers To start a batch pool and (optionally) install a set of specified Julia packages on the workers, we first need to create a bash script of the following form, which will be executed by each node joining the pool: #!/bin/bash ################################################################################################### # DO NOT MODIFY! # Switch to superuser and load module sudo bash pwd # Install Julia wget \"https://julialang-s3.julialang.org/bin/linux/x64/1.5/julia-1.5.2-linux-x86_64.tar.gz\" tar -xvzf julia-1.5.2-linux-x86_64.tar.gz rm -rf julia-1.5.2-linux-x86_64.tar.gz ln -s /mnt/batch/tasks/startup/wd/julia-1.5.2/bin/julia /usr/local/bin/julia # Install AzureClusterlessHPC git clone https://github.com/microsoft/AzureClusterlessHPC.jl julia -e 'using Pkg; Pkg.add(url=joinpath(pwd(), \"AzureClusterlessHPC\"))' ################################################################################################### # ADD USER PACKAGES HERE # ... ################################################################################################### # DO NOT MODIFY! # Make julia dir available for all users chmod -R 777 /mnt/batch/tasks/startup/wd/.julia If you need to install Julia packages for your application, specify the packages in the section # ADD USER PACKAGES HERE . E.g. to install the Julia package IterativeSolvers.jl , add the line: julia -e 'using Pkg; Pkg.add(\"IterativeSolvers\")' To install packages that are not officially registered with Julia, use this line to add packages: julia -e 'using Pkg; Pkg.develop(PackageSpec(url=\"https://github.com/slimgroup/JOLI.jl\"))' Save this batch script, e.g. as pool_startup_script.sh . You can now create a pool in which the startup script will be executed on each node that joins the pool: # Path to bash file startup_script = \"/path/to/pool_startup_script.sh\" # Create pool create_pool_and_resource_file(startup_script; enable_auto_scale=false, auto_scale_formula=nothing, auto_scale_evaluation_interval_minutes=nothing, image_resource_id=nothing) Required input arguments: startup_script : String that defines the path and name of the bash startup script. Optional keyword arguments : enable_auto_scale=false : Enable auto scaling. If true , the keyword arguments auto_scale_formula and auto_scale_evaluation_interval_minutes must be provided as well. If the parameter _POOL_NODE_COUNT has been set, it will be ignored. auto_scale_formula=nothing : String that defines the auto-scaling behavior. See here for Azure Batch auto-scaling templates. auto_scale_evaluation_interval_minutes=nothing : Time interval between evaluations of the auto-scaling function. The minimum possible interval is 5 minutes. image_resource_id=nothing : Provide an optional image resource ID to use a custom machine image for nodes joining the batch pool. Start a pool using an existing VM image To launch a pool with a custom VM image, you need to create a custom VM image and then upload it to the Azure shared image gallery. The image gallery will assign an image reference ID to the image (see here for details on how to create a shared image). Once you have the shared image ID, pass it as a keyword argument image_resource_id to the create_pool function. If you do not pass the image ID to the function, workers are created with the default Ubuntu image, which does not have Julia installed. # Image resource ID image_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # Create pool with custom VM image create_pool(image_resource_id=image_id, enable_auto_scale=false, auto_scale_formula=nothing, auto_scale_evaluation_interval_minutes=nothing) Optional keyword arguments: image_resource_id=nothing : Image resource ID to use a custom machine image for nodes joining the batch pool. For a description of all other keyword arguments, see the above section. Important : In your parameter file, set the variable \"_JULIA_DEPOT_PATH\" to the path where Julia is installed on the image. Start a pool using a Docker image As a third alternative, you can create an application package using Docker. You first create or specify a Docker image, which will then be pre-installed on each VM joining the batch pool. See the example directory /path/to/redwood/examples/container for an example Dockerfile. Follow the subsequent instructions to create a Docker image from a Dockerfile and upload it to your (personal) container repository: # Move to directory with Dockerfile cd /path/to/redwood/examples/container # Build image docker build -t redoowd:v1.0 . # Login docker login # Tag and push docker tag redwood:v1.0 username/redwood:v1.0 docker push username/redwood:v1.0 Once you have a Docker image in a public repository, you can specify a Docker image in your parameters.json file: \"_CONTAINER\": \"username/redwood:v1.0\" If the _CONTAINER parameter is set, AzureClusterlessHPC will install the specified container image on the VMs in the batch pool. Pools with auto-scaling To create a pool with auto-scaling, use one of the above commands and set the following keyword arguments: Set the keyword argument enable_auto_scale=true Define an auto-scaling formula. E.g. the following formula creates a pool with 1 node and resizes the pool to up to 10 VMs based on the number of pending tasks: auto_scale_formula = \"\"\"startingNumberOfVMs = 1; maxNumberofVMs = 10; pendingTaskSamplePercent = \\$PendingTasks.GetSamplePercent(30 * TimeInterval_Second); pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg(\\$PendingTasks.GetSample(30 * TimeInterval_Second)); \\$TargetDedicatedNodes=min(maxNumberofVMs, pendingTaskSamples); \\$NodeDeallocationOption = taskcompletion;\"\"\" For other auto-scaling formulas, refer to the Azure Batch documentation . Set the auto-scaling interval: auto_scale_evaluation_interval_minutes=5 . The minimum allowed values is 5 minutes. The full example would look like this: # Pool startup script startup_script = \"/path/to/pool_startup_script.sh\" # Autoscale formula auto_scale_formula = \"\"\"startingNumberOfVMs = 1; maxNumberofVMs = 10; pendingTaskSamplePercent = \\$PendingTasks.GetSamplePercent(30 * TimeInterval_Second); pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg(\\$PendingTasks.GetSample(30 * TimeInterval_Second)); \\$TargetDedicatedNodes=min(maxNumberofVMs, pendingTaskSamples); \\$NodeDeallocationOption = taskcompletion;\"\"\" create_pool_and_resource_file(startup_script; enable_auto_scale=true, auto_scale_formula=auto_scale_formula, auto_scale_evaluation_interval_minutes=5) Resize the pool Currently not supported.","title":"Manage pools"},{"location":"pool/#set-up-a-batch-pool","text":"","title":"Set up a batch pool"},{"location":"pool/#start-a-pool-and-optionally-install-julia-packages-on-the-workers","text":"To start a batch pool and (optionally) install a set of specified Julia packages on the workers, we first need to create a bash script of the following form, which will be executed by each node joining the pool: #!/bin/bash ################################################################################################### # DO NOT MODIFY! # Switch to superuser and load module sudo bash pwd # Install Julia wget \"https://julialang-s3.julialang.org/bin/linux/x64/1.5/julia-1.5.2-linux-x86_64.tar.gz\" tar -xvzf julia-1.5.2-linux-x86_64.tar.gz rm -rf julia-1.5.2-linux-x86_64.tar.gz ln -s /mnt/batch/tasks/startup/wd/julia-1.5.2/bin/julia /usr/local/bin/julia # Install AzureClusterlessHPC git clone https://github.com/microsoft/AzureClusterlessHPC.jl julia -e 'using Pkg; Pkg.add(url=joinpath(pwd(), \"AzureClusterlessHPC\"))' ################################################################################################### # ADD USER PACKAGES HERE # ... ################################################################################################### # DO NOT MODIFY! # Make julia dir available for all users chmod -R 777 /mnt/batch/tasks/startup/wd/.julia If you need to install Julia packages for your application, specify the packages in the section # ADD USER PACKAGES HERE . E.g. to install the Julia package IterativeSolvers.jl , add the line: julia -e 'using Pkg; Pkg.add(\"IterativeSolvers\")' To install packages that are not officially registered with Julia, use this line to add packages: julia -e 'using Pkg; Pkg.develop(PackageSpec(url=\"https://github.com/slimgroup/JOLI.jl\"))' Save this batch script, e.g. as pool_startup_script.sh . You can now create a pool in which the startup script will be executed on each node that joins the pool: # Path to bash file startup_script = \"/path/to/pool_startup_script.sh\" # Create pool create_pool_and_resource_file(startup_script; enable_auto_scale=false, auto_scale_formula=nothing, auto_scale_evaluation_interval_minutes=nothing, image_resource_id=nothing) Required input arguments: startup_script : String that defines the path and name of the bash startup script. Optional keyword arguments : enable_auto_scale=false : Enable auto scaling. If true , the keyword arguments auto_scale_formula and auto_scale_evaluation_interval_minutes must be provided as well. If the parameter _POOL_NODE_COUNT has been set, it will be ignored. auto_scale_formula=nothing : String that defines the auto-scaling behavior. See here for Azure Batch auto-scaling templates. auto_scale_evaluation_interval_minutes=nothing : Time interval between evaluations of the auto-scaling function. The minimum possible interval is 5 minutes. image_resource_id=nothing : Provide an optional image resource ID to use a custom machine image for nodes joining the batch pool.","title":"Start a pool and optionally install Julia packages on the workers"},{"location":"pool/#start-a-pool-using-an-existing-vm-image","text":"To launch a pool with a custom VM image, you need to create a custom VM image and then upload it to the Azure shared image gallery. The image gallery will assign an image reference ID to the image (see here for details on how to create a shared image). Once you have the shared image ID, pass it as a keyword argument image_resource_id to the create_pool function. If you do not pass the image ID to the function, workers are created with the default Ubuntu image, which does not have Julia installed. # Image resource ID image_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # Create pool with custom VM image create_pool(image_resource_id=image_id, enable_auto_scale=false, auto_scale_formula=nothing, auto_scale_evaluation_interval_minutes=nothing) Optional keyword arguments: image_resource_id=nothing : Image resource ID to use a custom machine image for nodes joining the batch pool. For a description of all other keyword arguments, see the above section. Important : In your parameter file, set the variable \"_JULIA_DEPOT_PATH\" to the path where Julia is installed on the image.","title":"Start a pool using an existing VM image"},{"location":"pool/#start-a-pool-using-a-docker-image","text":"As a third alternative, you can create an application package using Docker. You first create or specify a Docker image, which will then be pre-installed on each VM joining the batch pool. See the example directory /path/to/redwood/examples/container for an example Dockerfile. Follow the subsequent instructions to create a Docker image from a Dockerfile and upload it to your (personal) container repository: # Move to directory with Dockerfile cd /path/to/redwood/examples/container # Build image docker build -t redoowd:v1.0 . # Login docker login # Tag and push docker tag redwood:v1.0 username/redwood:v1.0 docker push username/redwood:v1.0 Once you have a Docker image in a public repository, you can specify a Docker image in your parameters.json file: \"_CONTAINER\": \"username/redwood:v1.0\" If the _CONTAINER parameter is set, AzureClusterlessHPC will install the specified container image on the VMs in the batch pool.","title":"Start a pool using a Docker image"},{"location":"pool/#pools-with-auto-scaling","text":"To create a pool with auto-scaling, use one of the above commands and set the following keyword arguments: Set the keyword argument enable_auto_scale=true Define an auto-scaling formula. E.g. the following formula creates a pool with 1 node and resizes the pool to up to 10 VMs based on the number of pending tasks: auto_scale_formula = \"\"\"startingNumberOfVMs = 1; maxNumberofVMs = 10; pendingTaskSamplePercent = \\$PendingTasks.GetSamplePercent(30 * TimeInterval_Second); pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg(\\$PendingTasks.GetSample(30 * TimeInterval_Second)); \\$TargetDedicatedNodes=min(maxNumberofVMs, pendingTaskSamples); \\$NodeDeallocationOption = taskcompletion;\"\"\" For other auto-scaling formulas, refer to the Azure Batch documentation . Set the auto-scaling interval: auto_scale_evaluation_interval_minutes=5 . The minimum allowed values is 5 minutes. The full example would look like this: # Pool startup script startup_script = \"/path/to/pool_startup_script.sh\" # Autoscale formula auto_scale_formula = \"\"\"startingNumberOfVMs = 1; maxNumberofVMs = 10; pendingTaskSamplePercent = \\$PendingTasks.GetSamplePercent(30 * TimeInterval_Second); pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg(\\$PendingTasks.GetSample(30 * TimeInterval_Second)); \\$TargetDedicatedNodes=min(maxNumberofVMs, pendingTaskSamples); \\$NodeDeallocationOption = taskcompletion;\"\"\" create_pool_and_resource_file(startup_script; enable_auto_scale=true, auto_scale_formula=auto_scale_formula, auto_scale_evaluation_interval_minutes=5)","title":"Pools with auto-scaling"},{"location":"pool/#resize-the-pool","text":"Currently not supported.","title":"Resize the pool"},{"location":"quickstart/","text":"Quick start Before running an example, we need to create two JSON files with our Azure credentials and the job parameters, as well as a bash startup-script for the worker nodes. Templates for these files are located in the examples directory: # Go to example directory cd /path/to/AzureClusterlessHPC/examples/batch # List directory content ls -l credentials.json julia_batch_macros.ipynb parameters.json pool_startup_script.sh Fill out the missing information in credentials.json and in parameters.json (see the next section \"Parmeters and credentials\" for additional information). Then set the environment variables CREDENTIALS and PARAMETERS so that they point to the files. You can either set the variables in your bash terminal (e.g. in your ~/.bashrc file), or directly in the Julia terminal: # Set path to credentials in Julia ENV[\"CREDENTIALS\"] = joinpath(pwd(), \"credentials.json\") # Set path to batch parameters (pool id, VM types, etc.) ENV[\"PARAMETERS\"] = joinpath(pwd(), \"parameters.json\") Next, load AzureClusterlessHPC.jl and create a pool with the parameters from parameters.json : # Load package using AzureClusterlessHPC # Create default pool with parameters from parameters.json startup_script = \"pool_startup_script.sh\" create_pool_and_resource_file(startup_script) Remark: If a pool with the name as specified in parameter.json already exists, the create_pool_and_resource_file function will throw an error.In practice, use a try ... catch block around this expression. Now you can execute Julia functions that are defined using the @batchdef macro via Azure batch: # Define function @batchdef function hello_world(name) print(\"Hello $name\") return \"Goodbye\" end # Execute function via Azure batch @batchexec hello_world(\"Bob\") You can also run multi-tasks batch job using the pmap function in combination with @batchdef : # Run a multi-task batch job @batchexec pmap(name -> hello_world(name), [\"Bob\", \"Jane\"]) To delete all resources run: # Shut down pool delete_pool() # Delete container with temporary blob files delete_container()","title":"Quickstart"},{"location":"quickstart/#quick-start","text":"Before running an example, we need to create two JSON files with our Azure credentials and the job parameters, as well as a bash startup-script for the worker nodes. Templates for these files are located in the examples directory: # Go to example directory cd /path/to/AzureClusterlessHPC/examples/batch # List directory content ls -l credentials.json julia_batch_macros.ipynb parameters.json pool_startup_script.sh Fill out the missing information in credentials.json and in parameters.json (see the next section \"Parmeters and credentials\" for additional information). Then set the environment variables CREDENTIALS and PARAMETERS so that they point to the files. You can either set the variables in your bash terminal (e.g. in your ~/.bashrc file), or directly in the Julia terminal: # Set path to credentials in Julia ENV[\"CREDENTIALS\"] = joinpath(pwd(), \"credentials.json\") # Set path to batch parameters (pool id, VM types, etc.) ENV[\"PARAMETERS\"] = joinpath(pwd(), \"parameters.json\") Next, load AzureClusterlessHPC.jl and create a pool with the parameters from parameters.json : # Load package using AzureClusterlessHPC # Create default pool with parameters from parameters.json startup_script = \"pool_startup_script.sh\" create_pool_and_resource_file(startup_script) Remark: If a pool with the name as specified in parameter.json already exists, the create_pool_and_resource_file function will throw an error.In practice, use a try ... catch block around this expression. Now you can execute Julia functions that are defined using the @batchdef macro via Azure batch: # Define function @batchdef function hello_world(name) print(\"Hello $name\") return \"Goodbye\" end # Execute function via Azure batch @batchexec hello_world(\"Bob\") You can also run multi-tasks batch job using the pmap function in combination with @batchdef : # Run a multi-task batch job @batchexec pmap(name -> hello_world(name), [\"Bob\", \"Jane\"]) To delete all resources run: # Shut down pool delete_pool() # Delete container with temporary blob files delete_container()","title":"Quick start"}]}