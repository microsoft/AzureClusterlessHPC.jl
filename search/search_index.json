{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AzureClusterlessHPC.jl Overview AzureClusterlessHPC.jl is a package for simplified parallel computing in the cloud. AzureClusterlessHPC.jl borrows the syntax of Julia's Distributed Programming package to easily execute parallel Julia workloads in the cloud. Unlike traditional distributed programming in Julia, you do not need create a cluster of interconnected nodes with a parallel Julia session running on them. Instead, AzureClusterlessHPC.jl allows you to create one or multiple pools of virtual machines (VMs) on which you can remotely execute Julia functions: The advantage is that you can scale almost indefinitely, as you elliminate the master worker as a bottleneck that needs to communicate with an increasing number of workers. Instead, you run on a single Julia worker that schedulers and executes workloads through one or multiple Azure Batch clients. The underlying resources are managed by the cloud and can dynamically shrink and grow at any time: Each pool can have up to 2,000 individual nodes or up to 100 interconnected nodes. Pools with interconnected nodes allow you to run MPI or distributed Julia code within a single pool and you can run multiple MPI function across many pools. Troubleshooting If you encounter any problems or errors, please file a github issue here .","title":"Home"},{"location":"#azureclusterlesshpcjl","text":"","title":"AzureClusterlessHPC.jl"},{"location":"#overview","text":"AzureClusterlessHPC.jl is a package for simplified parallel computing in the cloud. AzureClusterlessHPC.jl borrows the syntax of Julia's Distributed Programming package to easily execute parallel Julia workloads in the cloud. Unlike traditional distributed programming in Julia, you do not need create a cluster of interconnected nodes with a parallel Julia session running on them. Instead, AzureClusterlessHPC.jl allows you to create one or multiple pools of virtual machines (VMs) on which you can remotely execute Julia functions: The advantage is that you can scale almost indefinitely, as you elliminate the master worker as a bottleneck that needs to communicate with an increasing number of workers. Instead, you run on a single Julia worker that schedulers and executes workloads through one or multiple Azure Batch clients. The underlying resources are managed by the cloud and can dynamically shrink and grow at any time: Each pool can have up to 2,000 individual nodes or up to 100 interconnected nodes. Pools with interconnected nodes allow you to run MPI or distributed Julia code within a single pool and you can run multiple MPI function across many pools.","title":"Overview"},{"location":"#troubleshooting","text":"If you encounter any problems or errors, please file a github issue here .","title":"Troubleshooting"},{"location":"about/","text":"About AzureClusterlessHPC.jl is developed and maintained by the Microsoft Research for Industry (RFI) team. Copyright Copyright (c) Microsoft Corporation. All rights reserved. Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.","title":"About"},{"location":"about/#about","text":"AzureClusterlessHPC.jl is developed and maintained by the Microsoft Research for Industry (RFI) team.","title":"About"},{"location":"about/#copyright","text":"Copyright (c) Microsoft Corporation. All rights reserved. Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.","title":"Copyright"},{"location":"broadcasting/","text":"Broadcasting Broadcast an expression to all batch workers of (future) batch jobs and return a batch future. The batch future can be passed as a function argument instead of the variable. batch_future = @bcast expr The use of @bcast is recommended to pass large arguments to functions (e.g. arrays). This avoids copying input arguments to each individual task separately. Instead, expressions tagged via @batchdef are uploaded to blob storage once and their blob reference is passed to one or multiple tasks. To access a broadcasted variable inside an executed function, use the fetch or fetch! (in-place) function: # Create and broadcast array A = randn(2, 2) _A = @bcast A # Define function @batchdef function print_array(_A) A = fetch(_A) # load A into memory print(A) end # Remotely execute function @batchexec print_array(_A) # pass batch future Calling A = fetch(_A) on the local machine (rather than on a batch worker) downloads the broadcasted variable from blob storage and returns it.","title":"Broadcasting"},{"location":"broadcasting/#broadcasting","text":"Broadcast an expression to all batch workers of (future) batch jobs and return a batch future. The batch future can be passed as a function argument instead of the variable. batch_future = @bcast expr The use of @bcast is recommended to pass large arguments to functions (e.g. arrays). This avoids copying input arguments to each individual task separately. Instead, expressions tagged via @batchdef are uploaded to blob storage once and their blob reference is passed to one or multiple tasks. To access a broadcasted variable inside an executed function, use the fetch or fetch! (in-place) function: # Create and broadcast array A = randn(2, 2) _A = @bcast A # Define function @batchdef function print_array(_A) A = fetch(_A) # load A into memory print(A) end # Remotely execute function @batchexec print_array(_A) # pass batch future Calling A = fetch(_A) on the local machine (rather than on a batch worker) downloads the broadcasted variable from blob storage and returns it.","title":"Broadcasting"},{"location":"cleanup/","text":"Clean up resources After executing a batch job via @batchexec , you can use the returned batch controller to clean up resources: # Batch job batch_controller = @batchexec print(\"Hello world\") # Terminate job terminate_job(batch_controller) # Delete job delete_job(batch_controller) # Delete pool delete_pool(batch_controller) # Delete blob container with all temporary files delte_container(batch_controller) # Or alternatively, delete pool + job + container together destroy!(batch_controller) If you did not return a batch controller, you can call the following functions without any input arguments, in which case they will delete the pool and container as specified in your parameter.json file (or the default ones). The delete_all_jobs function will delete all exisiting jobs that start with the job id specified in the parameter file. # Delete container delete_container() # Delete pool delete_pool() # Delete all jobs delete_all_jobs()","title":"Clean up"},{"location":"cleanup/#clean-up-resources","text":"After executing a batch job via @batchexec , you can use the returned batch controller to clean up resources: # Batch job batch_controller = @batchexec print(\"Hello world\") # Terminate job terminate_job(batch_controller) # Delete job delete_job(batch_controller) # Delete pool delete_pool(batch_controller) # Delete blob container with all temporary files delte_container(batch_controller) # Or alternatively, delete pool + job + container together destroy!(batch_controller) If you did not return a batch controller, you can call the following functions without any input arguments, in which case they will delete the pool and container as specified in your parameter.json file (or the default ones). The delete_all_jobs function will delete all exisiting jobs that start with the job id specified in the parameter file. # Delete container delete_container() # Delete pool delete_pool() # Delete all jobs delete_all_jobs()","title":"Clean up resources"},{"location":"credentials/","text":"Parameters and credentials Single batch and storage account AzureClusterlessHPC.jl requires at the minimum one Azure Batch account and one Azure Storage account. The storage account is authenticated via a secret key, whereas Azure Batch must be authenticated using the Azure Active Directory (AAD). The setup.sh script in the root directory automatically creates the accounts and creates a credentials.json file. Alternatively, you can set up the accounts manually (e.g. using the Azure console). Refer to the Azure documentation for information on how to authenticate Azure Batch via the AAD. Once you have created the accounts, create a credential file with the storage key and batch AAD authentication. Use the following template for the credentials.json file: { \"_AD_TENANT\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"_AD_BATCH_CLIENT_ID\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"_AD_SECRET_BATCH\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\", \"_BATCH_ACCOUNT_URL\": \"https://batchaccountname.batchregion.batch.azure.com\", \"_BATCH_RESOURCE\": \"https://batch.core.windows.net/\", \"_STORAGE_ACCOUNT_NAME\": \"storageaccountname\", \"_STORAGE_ACCOUNT_KEY\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" } When using AzureClusterlessHPC, set the environment variable ENV[\"CREDENTIALS\"] = \"/path/to/credentials.json\" before you load the package via using AzureClusterlessHPC . Job monitoring via App Insights AzureClusterlessHPC.jl supports job monitoring via Azure Batch Insights. Follow the Batch Insights instructions to set up Applications insights in the Azure portal. From your application, retrieve the app insights ID and the instrumentation key and add them to your credentials.json file: { \"_APP_INSIGHTS_APP_ID\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"_APP_INSIGHTS_INSTRUMENTATION_KEY\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" } Multi accounts AzureClusterlessHPC also allows using multiple storage and/or batch accounts. Using multiple batch accounts provides the possiblity to cirumvent service limits of a single batch account or it allows to distribute workloads among multiple regions. If you create batch accounts for multiple regions, you need to have at least one storage account in each region. To automatically create multiple batch and storage accounts, use the shell script create_azure_accounts.sh . Pass the list of region(s) and the number of accounts per region as command line arguments to the script. E.g., to create two batch and storeage acounts in each US West and South Central US (i.e, total of 4 batch and 4 storage accounts), run: # Go to AzureClusterlessHPC directory cd /path/to/AzureClusterlessHPC # Azure CLI log in az login # Create accounts ./create_azure_accounts \"westus southcentralus\" 2 Creating the accounts may take several minutes, depending on how many accounts are being created. The script also fetches the required credentials and stores them in the directory user_data . No further actions from the user side are required. To use the credentials stored in user_data with AzureClusterlessHPC, make sure that the environment variable \"CREDENTIALS\" is unset (run unset CREDENTIALS from the bash command line). If CREDENTIALS is not set, AzureClusterlessHPC will automatically look for credentials in user_data . After loading AzureClusterlessHPC in Julia ( using AzureClusterlessHPC ), you can check which accounts were found by checking AzureClusterlessHPC.__credentials__ . This returns a list with one entry per available batch account. Type AzureClusterlessHPC.__credentials__[i] to print the credential information for the i-th account. User parameters Users can optionally provide a parameters.json file that specifies pool and job parameters. Set the environment variable ENV[\"PARAMETERS\"]=/path/to/parameters.json before loading the package (see section \"Quickstart\" for an example). The following set of parameters and default values are used, unless specified otherwise by the user: { \"_POOL_ID\": \"BatchPool\", \"_POOL_COUNT\": \"1\", \"_NODE_COUNT_PER_POOL\": \"1\", \"_POOL_VM_SIZE\": \"Standard_E2s_v3\", \"_JOB_ID\": \"BatchJob\", \"_STANDARD_OUT_FILE_NAME\": \"stdout.txt\", \"_NODE_OS_PUBLISHER\": \"Canonical\", \"_NODE_OS_OFFER\": \"UbuntuServer\", \"_NODE_OS_SKU\": \"18.04\", \"_BLOB_CONTAINER\": \"redwoodtemp\", \"_INTER_NODE_CONNECTION\": \"0\", \"_NUM_RETRYS\": \"0\", \"_MPI_RUN\": \"0\", \"_CONTAINER\": \"None\", \"_NUM_NODES_PER_TASK\": \"1\", \"_NUM_PROCS_PER_NODE\": \"1\", \"_OMP_NUM_THREADS\": \"1\", \"_JULIA_DEPOT_PATH\": \"/mnt/batch/tasks/startup/wd/.julia\", \"_PYTHONPATH\": \"/mnt/batch/tasks/startup/wd/.local/lib/python3.6/site-packages\" } Note: Do not modify the \"_JULIA_DEPOT_PATH\" and \"_PYTHONPATH\" unless you use a pool with a custom image or Docker container in which Julia has been already installed. In that case, set the depot path to the location of the .julia directory.","title":"Credentials"},{"location":"credentials/#parameters-and-credentials","text":"","title":"Parameters and credentials"},{"location":"credentials/#single-batch-and-storage-account","text":"AzureClusterlessHPC.jl requires at the minimum one Azure Batch account and one Azure Storage account. The storage account is authenticated via a secret key, whereas Azure Batch must be authenticated using the Azure Active Directory (AAD). The setup.sh script in the root directory automatically creates the accounts and creates a credentials.json file. Alternatively, you can set up the accounts manually (e.g. using the Azure console). Refer to the Azure documentation for information on how to authenticate Azure Batch via the AAD. Once you have created the accounts, create a credential file with the storage key and batch AAD authentication. Use the following template for the credentials.json file: { \"_AD_TENANT\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"_AD_BATCH_CLIENT_ID\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"_AD_SECRET_BATCH\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\", \"_BATCH_ACCOUNT_URL\": \"https://batchaccountname.batchregion.batch.azure.com\", \"_BATCH_RESOURCE\": \"https://batch.core.windows.net/\", \"_STORAGE_ACCOUNT_NAME\": \"storageaccountname\", \"_STORAGE_ACCOUNT_KEY\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" } When using AzureClusterlessHPC, set the environment variable ENV[\"CREDENTIALS\"] = \"/path/to/credentials.json\" before you load the package via using AzureClusterlessHPC .","title":"Single batch and storage account"},{"location":"credentials/#job-monitoring-via-app-insights","text":"AzureClusterlessHPC.jl supports job monitoring via Azure Batch Insights. Follow the Batch Insights instructions to set up Applications insights in the Azure portal. From your application, retrieve the app insights ID and the instrumentation key and add them to your credentials.json file: { \"_APP_INSIGHTS_APP_ID\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", \"_APP_INSIGHTS_INSTRUMENTATION_KEY\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }","title":"Job monitoring via App Insights"},{"location":"credentials/#multi-accounts","text":"AzureClusterlessHPC also allows using multiple storage and/or batch accounts. Using multiple batch accounts provides the possiblity to cirumvent service limits of a single batch account or it allows to distribute workloads among multiple regions. If you create batch accounts for multiple regions, you need to have at least one storage account in each region. To automatically create multiple batch and storage accounts, use the shell script create_azure_accounts.sh . Pass the list of region(s) and the number of accounts per region as command line arguments to the script. E.g., to create two batch and storeage acounts in each US West and South Central US (i.e, total of 4 batch and 4 storage accounts), run: # Go to AzureClusterlessHPC directory cd /path/to/AzureClusterlessHPC # Azure CLI log in az login # Create accounts ./create_azure_accounts \"westus southcentralus\" 2 Creating the accounts may take several minutes, depending on how many accounts are being created. The script also fetches the required credentials and stores them in the directory user_data . No further actions from the user side are required. To use the credentials stored in user_data with AzureClusterlessHPC, make sure that the environment variable \"CREDENTIALS\" is unset (run unset CREDENTIALS from the bash command line). If CREDENTIALS is not set, AzureClusterlessHPC will automatically look for credentials in user_data . After loading AzureClusterlessHPC in Julia ( using AzureClusterlessHPC ), you can check which accounts were found by checking AzureClusterlessHPC.__credentials__ . This returns a list with one entry per available batch account. Type AzureClusterlessHPC.__credentials__[i] to print the credential information for the i-th account.","title":"Multi accounts"},{"location":"credentials/#user-parameters","text":"Users can optionally provide a parameters.json file that specifies pool and job parameters. Set the environment variable ENV[\"PARAMETERS\"]=/path/to/parameters.json before loading the package (see section \"Quickstart\" for an example). The following set of parameters and default values are used, unless specified otherwise by the user: { \"_POOL_ID\": \"BatchPool\", \"_POOL_COUNT\": \"1\", \"_NODE_COUNT_PER_POOL\": \"1\", \"_POOL_VM_SIZE\": \"Standard_E2s_v3\", \"_JOB_ID\": \"BatchJob\", \"_STANDARD_OUT_FILE_NAME\": \"stdout.txt\", \"_NODE_OS_PUBLISHER\": \"Canonical\", \"_NODE_OS_OFFER\": \"UbuntuServer\", \"_NODE_OS_SKU\": \"18.04\", \"_BLOB_CONTAINER\": \"redwoodtemp\", \"_INTER_NODE_CONNECTION\": \"0\", \"_NUM_RETRYS\": \"0\", \"_MPI_RUN\": \"0\", \"_CONTAINER\": \"None\", \"_NUM_NODES_PER_TASK\": \"1\", \"_NUM_PROCS_PER_NODE\": \"1\", \"_OMP_NUM_THREADS\": \"1\", \"_JULIA_DEPOT_PATH\": \"/mnt/batch/tasks/startup/wd/.julia\", \"_PYTHONPATH\": \"/mnt/batch/tasks/startup/wd/.local/lib/python3.6/site-packages\" } Note: Do not modify the \"_JULIA_DEPOT_PATH\" and \"_PYTHONPATH\" unless you use a pool with a custom image or Docker container in which Julia has been already installed. In that case, set the depot path to the location of the .julia directory.","title":"User parameters"},{"location":"faq/","text":"FAQ How does AzureClusterlessHPC work? Whenever you tag an expression with @batchdef , AzureClusterlessHPC collects the abstract syntax tree (AST) of the expressions and appends it to a global collection. You can print the currently collected AST via batch_show() and you can reset the collected expressions via batch_clear() . When you use @batchexec , AzureClusterlessHPC creates a closure around the executed expression and uploads it, along with the collected AST as a batch resource file. AzureClusterlessHPC also anayzes the executed funtion and replaces return statements with serializations, so that return arguments are written to the local disk of the batch worker and subsequently uploaded to blob storage, from where they can be collected via the fetch / fetch! functions. What costs does AzureClusterlessHPC incur? AzureClusterlessHPC calls Azure Batch and Azure Blob Storage APIs. Costs incur for operations that write data to blob storage, download or store it (e.g. @bcast , @batchexec , fetch , fetch! ). For batch jobs, costs incur for the requested VMs in the batch pool (regardless of whether jobs are currently running or not). How do I clean up and shut down all services that invoke costs? Costs are invoked by a batch pool made up of one or multiple VMs and by files stored in blob storage. To shut down the pool run delete_pool and to delete the blob container that contains any temporary files run delete_container() . These actions will delete the pool and blob container specified in your parameter JSON file (or the default ones created by AzureClusterlessHPC). How can I specify Julia packages to be installed on the batch worker nodes? To specify Julia packages that are installed on the worker nodes, create a pool startup script and use the create_pool_and_resource_file function to launch the pool. Refer to the section \"Create a batch pool\" for details. How can I start a pool with a custom VM image? To start a pool with a custom VM image, you need to first create a custom VM image and then upload it to the Azure shared image gallery. The image gallery will assign an image reference ID to the image (see here for details on how to create a shared image). When starting your batch pool, pass this ID to the pool startup function: create_pool(image_resource_id=\"shared_image_id\") . What kind of input and return arguments are supported in functions executed via @batchexec ? AzureClusterlessHPC.jl supports any kind of input and return arguments, including custom data structures. Input and return arguments do not need to be JSON serializable. However, we recommend using the same Julia version on the batch workers as on your local machine or master VM. This avoids possible inconsistencies when serializing/deserializing arguments and expressions. Are MPI and multi-node batch tasks supported? Yes, you can execute AzureClusterlessHPC tasks via Julia MPI on either single VMs or on multiple VMs. See the above section MPI support for details on how to runs batch tasks with MPI support.","title":"FAQ"},{"location":"faq/#faq","text":"How does AzureClusterlessHPC work? Whenever you tag an expression with @batchdef , AzureClusterlessHPC collects the abstract syntax tree (AST) of the expressions and appends it to a global collection. You can print the currently collected AST via batch_show() and you can reset the collected expressions via batch_clear() . When you use @batchexec , AzureClusterlessHPC creates a closure around the executed expression and uploads it, along with the collected AST as a batch resource file. AzureClusterlessHPC also anayzes the executed funtion and replaces return statements with serializations, so that return arguments are written to the local disk of the batch worker and subsequently uploaded to blob storage, from where they can be collected via the fetch / fetch! functions. What costs does AzureClusterlessHPC incur? AzureClusterlessHPC calls Azure Batch and Azure Blob Storage APIs. Costs incur for operations that write data to blob storage, download or store it (e.g. @bcast , @batchexec , fetch , fetch! ). For batch jobs, costs incur for the requested VMs in the batch pool (regardless of whether jobs are currently running or not). How do I clean up and shut down all services that invoke costs? Costs are invoked by a batch pool made up of one or multiple VMs and by files stored in blob storage. To shut down the pool run delete_pool and to delete the blob container that contains any temporary files run delete_container() . These actions will delete the pool and blob container specified in your parameter JSON file (or the default ones created by AzureClusterlessHPC). How can I specify Julia packages to be installed on the batch worker nodes? To specify Julia packages that are installed on the worker nodes, create a pool startup script and use the create_pool_and_resource_file function to launch the pool. Refer to the section \"Create a batch pool\" for details. How can I start a pool with a custom VM image? To start a pool with a custom VM image, you need to first create a custom VM image and then upload it to the Azure shared image gallery. The image gallery will assign an image reference ID to the image (see here for details on how to create a shared image). When starting your batch pool, pass this ID to the pool startup function: create_pool(image_resource_id=\"shared_image_id\") . What kind of input and return arguments are supported in functions executed via @batchexec ? AzureClusterlessHPC.jl supports any kind of input and return arguments, including custom data structures. Input and return arguments do not need to be JSON serializable. However, we recommend using the same Julia version on the batch workers as on your local machine or master VM. This avoids possible inconsistencies when serializing/deserializing arguments and expressions. Are MPI and multi-node batch tasks supported? Yes, you can execute AzureClusterlessHPC tasks via Julia MPI on either single VMs or on multiple VMs. See the above section MPI support for details on how to runs batch tasks with MPI support.","title":"FAQ"},{"location":"files/","text":"Copying and retrieving files Aside from passing data and variables as function arguments to a batch task, it is possible to manually upload files from the local disk to a batch task. This avoids having to read a file into Julia before passing it as a function argument or broadcasting it. Similarily, it is possible to copy files from the batch worker disk back to the local disk. Upload files for batch task To upload a file from the local disk to your batch tasks, use the fileinclude function: @batchdef fileinclude(\"my_local_file.dat\") This expression first copies the file my_local_file.dat to the Azure blob store and then includes it as a resource file with any future batch task. Retrieve file from batch task If your remotely executed Julia function saves a file to the local batch worker disk, you can use the filereturn() function to move that file from the disk of the batch worker to blob storage: filereturn(\"my_remote_file.dat\") Note that the filereturn statement must be included in the function that will executed remotely as a batch task. See the following function as an example: @batchdef function create_file() # Do some work data = ... # Write output to local file iostream = open(\"my_remote_file.dat\", \"w\") write(iostream, data) # Copy remote file to blob store filereturn(\"my_remote_file.dat\") end Similar to return arguments, files from filereturn will be included as a Future in the batch controller. Once your function has executed successfully, you can retrieve the file via: # Remotely run function via azure batch bctrl = @batchexec create_file() # Wait for task to finish wait_for_tasks_to_complete(bctrl) # Copy \"my_remote_file.dat\" to your local disk fetch(bctrl.files; path=pwd())","title":"Copying and retrieving files"},{"location":"files/#copying-and-retrieving-files","text":"Aside from passing data and variables as function arguments to a batch task, it is possible to manually upload files from the local disk to a batch task. This avoids having to read a file into Julia before passing it as a function argument or broadcasting it. Similarily, it is possible to copy files from the batch worker disk back to the local disk.","title":"Copying and retrieving files"},{"location":"files/#upload-files-for-batch-task","text":"To upload a file from the local disk to your batch tasks, use the fileinclude function: @batchdef fileinclude(\"my_local_file.dat\") This expression first copies the file my_local_file.dat to the Azure blob store and then includes it as a resource file with any future batch task.","title":"Upload files for batch task"},{"location":"files/#retrieve-file-from-batch-task","text":"If your remotely executed Julia function saves a file to the local batch worker disk, you can use the filereturn() function to move that file from the disk of the batch worker to blob storage: filereturn(\"my_remote_file.dat\") Note that the filereturn statement must be included in the function that will executed remotely as a batch task. See the following function as an example: @batchdef function create_file() # Do some work data = ... # Write output to local file iostream = open(\"my_remote_file.dat\", \"w\") write(iostream, data) # Copy remote file to blob store filereturn(\"my_remote_file.dat\") end Similar to return arguments, files from filereturn will be included as a Future in the batch controller. Once your function has executed successfully, you can retrieve the file via: # Remotely run function via azure batch bctrl = @batchexec create_file() # Wait for task to finish wait_for_tasks_to_complete(bctrl) # Copy \"my_remote_file.dat\" to your local disk fetch(bctrl.files; path=pwd())","title":"Retrieve file from batch task"},{"location":"functioncalls/","text":"Remote function calls @batchdef Execute an expression under Main and on the batch workers of a (future) batch job that is executed from the same Julia session (equivalent to @everywhere for parallel Julia sessions). @batchdef expr @batchdef can be used to define variables, functions or with include and using statements: # Import packages @batchdef using LinearAlgebra, Random # Includes @batchdef include(\"testfile.jl\") # Define variables @batchdef A = ones(2, 2) # Define functions @batchdef hello_world(name) = print(\"Hello $name\") You can define multiple expression with @batchdef using a begin ... end block: @batchdef begin A = ones(1, 1) B = zeros(1, 1) end Expressions that are tagged via @batchdef are collected by AzureClusterlessHPC and are used in subsequent batch job executions. To print the current collection of expressions, type batch_show() . To reset the batch environment and remove all prior expressions from the call stack, use batch_clear() (or restart the Julia session). @batchexec Execute an expression as a batch job (equivalent to @spawn for parallel Julia sessions). @batchexec expr The primary purpose of @batchexec is to execute functions that have been priorly defined with @batchdef . E.g. # Define function @batchdef function hello_world(name) print(\"Hello $name\") return \"Goodbye\" end # Call function via batch bctrl = @batchexec hello_world(\"Bob\") Arguments for functions executed via @batchexec are always passed by copy . This is important to keep in mind when passing large arguments to a function that is executed as a multi-task batch job, in which case arguments are copied to each task separately. To pass large arguments to a multi-task batch job, use the @bcast macro (see next section). To execute a multi-task batch job, use the pmap function: # Multi-task batch job bctrl = @batchexec pmap(name -> hello_world(name), [\"Bob\", \"Jane\"]) The @batchexec macro returns a batch controller ( bctrl ) that can be used for the following actions: Wait for all tasks of the batch job to finish: wait_for_tasks_to_complete(bctrl) Terminate the batch job: terminate_job(bctrl) Delete the batch job: delete_job(bctrl) Delete the pool: delete_pool(bctrl) Delete the blob container in which all temporary files are stored: delete_container(bctrl) Destroy all Azure resources associated with the batch controller (job, pool, container): destroy!(bctrl) Fetch the output of all tasks: output = fetch(bctrl) . This operation is blocking and waits for all tasks to finish. The output is collected asynchonously in order of completion. Fetch the output of task i : output = fetch(bctrl, i) (blocking for that task). Inplace fetch (all tasks). Returns output and overwrites the blob future in bctrl.output : output = fetch!(bctrl) (blocking operation) Inplace fetch (task i ): output = fetch!(bctrl, i) (blocking for task i ) Fetch output of all tasks and apply a reduction operation to the output (along tasks): output_reduce = fetchreduce(bctrl; op=+) (blocking) Inplace fetch and reduce (overwrite output_reduce ): fetchreduce!(bctrl, output_reduce; op=+) (blocking) Limitations: Function return arguments must be explicitley returned via the return statement. I.e., implicit returns in which the final function expression is automatically returned are not supported. Functions executed via @batchexec can only have a single return argument. I.e. control structures such as if ... else ... end with multiple return statements are not supported and will throw an exception when fetching the output. Function arguments are passed by copy, never by reference. MPI support You can execute tasks via Julia MPI on either single VMs or on multiple VMs. To enable MPI on a single VM (shared memory parallelism), set the following variables in your parameters.json file: \"_INTER_NODE_CONNECTION\": \"0\", \"_MPI_RUN\": \"1\", \"_NUM_NODES_PER_TASK\": \"1\", \"_NUM_PROCS_PER_NODE\": \"2\", \"_OMP_NUM_THREADS\": \"1\" Note, that \"_NUM_NODES_PER_TASK\" must be set to 1 if \"_INTER_NODE_CONNECTION\" is set to \"0\" . \"_NUM_PROCS_PER_NODE\" specifies the number of MPI ranks per node and \"_OMP_NUM_THREADS\" specifies the number of OpenMP threads per rank (if applicable). To enable MPI tasks on multiple instances (distributed memory parallelism), set: \"_INTER_NODE_CONNECTION\": \"1\", \"_MPI_RUN\": \"1\", \"_NUM_NODES_PER_TASK\": \"2\", \"_NUM_PROCS_PER_NODE\": \"4\", \"_OMP_NUM_THREADS\": \"1\" The total number of MPI ranks for each task is given by \"_NUM_NODES_PER_TASK\" times \"_NUM_PROCS_PER_NODE\" . E.g. in this example, each MPI task is executed on 2 nodes with 4 processes per node, i.e. 8 MPI ranks in total. In your application, you need to load the Julia MPI package via @batchdef . For a full MPI example, see AzureClusterlessHPC/examples/mpi/julia_batch_mpi.ipynb .","title":"Remote function calls"},{"location":"functioncalls/#remote-function-calls","text":"","title":"Remote function calls"},{"location":"functioncalls/#batchdef","text":"Execute an expression under Main and on the batch workers of a (future) batch job that is executed from the same Julia session (equivalent to @everywhere for parallel Julia sessions). @batchdef expr @batchdef can be used to define variables, functions or with include and using statements: # Import packages @batchdef using LinearAlgebra, Random # Includes @batchdef include(\"testfile.jl\") # Define variables @batchdef A = ones(2, 2) # Define functions @batchdef hello_world(name) = print(\"Hello $name\") You can define multiple expression with @batchdef using a begin ... end block: @batchdef begin A = ones(1, 1) B = zeros(1, 1) end Expressions that are tagged via @batchdef are collected by AzureClusterlessHPC and are used in subsequent batch job executions. To print the current collection of expressions, type batch_show() . To reset the batch environment and remove all prior expressions from the call stack, use batch_clear() (or restart the Julia session).","title":"@batchdef"},{"location":"functioncalls/#batchexec","text":"Execute an expression as a batch job (equivalent to @spawn for parallel Julia sessions). @batchexec expr The primary purpose of @batchexec is to execute functions that have been priorly defined with @batchdef . E.g. # Define function @batchdef function hello_world(name) print(\"Hello $name\") return \"Goodbye\" end # Call function via batch bctrl = @batchexec hello_world(\"Bob\") Arguments for functions executed via @batchexec are always passed by copy . This is important to keep in mind when passing large arguments to a function that is executed as a multi-task batch job, in which case arguments are copied to each task separately. To pass large arguments to a multi-task batch job, use the @bcast macro (see next section). To execute a multi-task batch job, use the pmap function: # Multi-task batch job bctrl = @batchexec pmap(name -> hello_world(name), [\"Bob\", \"Jane\"]) The @batchexec macro returns a batch controller ( bctrl ) that can be used for the following actions: Wait for all tasks of the batch job to finish: wait_for_tasks_to_complete(bctrl) Terminate the batch job: terminate_job(bctrl) Delete the batch job: delete_job(bctrl) Delete the pool: delete_pool(bctrl) Delete the blob container in which all temporary files are stored: delete_container(bctrl) Destroy all Azure resources associated with the batch controller (job, pool, container): destroy!(bctrl) Fetch the output of all tasks: output = fetch(bctrl) . This operation is blocking and waits for all tasks to finish. The output is collected asynchonously in order of completion. Fetch the output of task i : output = fetch(bctrl, i) (blocking for that task). Inplace fetch (all tasks). Returns output and overwrites the blob future in bctrl.output : output = fetch!(bctrl) (blocking operation) Inplace fetch (task i ): output = fetch!(bctrl, i) (blocking for task i ) Fetch output of all tasks and apply a reduction operation to the output (along tasks): output_reduce = fetchreduce(bctrl; op=+) (blocking) Inplace fetch and reduce (overwrite output_reduce ): fetchreduce!(bctrl, output_reduce; op=+) (blocking) Limitations: Function return arguments must be explicitley returned via the return statement. I.e., implicit returns in which the final function expression is automatically returned are not supported. Functions executed via @batchexec can only have a single return argument. I.e. control structures such as if ... else ... end with multiple return statements are not supported and will throw an exception when fetching the output. Function arguments are passed by copy, never by reference.","title":"@batchexec"},{"location":"functioncalls/#mpi-support","text":"You can execute tasks via Julia MPI on either single VMs or on multiple VMs. To enable MPI on a single VM (shared memory parallelism), set the following variables in your parameters.json file: \"_INTER_NODE_CONNECTION\": \"0\", \"_MPI_RUN\": \"1\", \"_NUM_NODES_PER_TASK\": \"1\", \"_NUM_PROCS_PER_NODE\": \"2\", \"_OMP_NUM_THREADS\": \"1\" Note, that \"_NUM_NODES_PER_TASK\" must be set to 1 if \"_INTER_NODE_CONNECTION\" is set to \"0\" . \"_NUM_PROCS_PER_NODE\" specifies the number of MPI ranks per node and \"_OMP_NUM_THREADS\" specifies the number of OpenMP threads per rank (if applicable). To enable MPI tasks on multiple instances (distributed memory parallelism), set: \"_INTER_NODE_CONNECTION\": \"1\", \"_MPI_RUN\": \"1\", \"_NUM_NODES_PER_TASK\": \"2\", \"_NUM_PROCS_PER_NODE\": \"4\", \"_OMP_NUM_THREADS\": \"1\" The total number of MPI ranks for each task is given by \"_NUM_NODES_PER_TASK\" times \"_NUM_PROCS_PER_NODE\" . E.g. in this example, each MPI task is executed on 2 nodes with 4 processes per node, i.e. 8 MPI ranks in total. In your application, you need to load the Julia MPI package via @batchdef . For a full MPI example, see AzureClusterlessHPC/examples/mpi/julia_batch_mpi.ipynb .","title":"MPI support"},{"location":"installation/","text":"Installation and setup Using AzureClusterlessHPC requires at the minimum one Azure Batch and one Azure Blob Storage account. Before being able to run the example, follow the instructions from this section to install the package and dependencies, and set up the necessary Azure resources. Prerequisites Ubuntu 18.04/20.04 or Debian 10 Julia v1.5 or later Python3 and pip3 Important : Your local Julia version needs to match the Julia version that is installed on the batch workers. The examples in this repository are set up for Julia 1.6.1. Install Julia package Run the following command from an interactive Julia session to install the developer version of AzureClusterlessHPC.jl (press the ] key and then type the command) ] dev AzureClusterlessHPC.jl Install Python dependencies (only for custom Julia Python) If your Julia PyCall package uses the default Python version, all Python dependencies will be installed automatically. No actions are required. If you are using a custom Python version for PyCall, the Python dependencies need to be install manually into the Python environment that PyCall is using. E.g.: # Go to AzureClusterlessHPC directory cd ~/.julia/dev/AzureClusterlessHPC pip3 install -r pyrequirements.txt Create Azure Storage and Batch account with AAD authentication Using AzureClusterlessHPC.jl requires an Azure Storage and Azure Batch account with AAD authentication. First, install the Azure Command Line Interface (CLI) by running (see here for additional instructions): curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash Next, log into your Azure account: az login Once you have sucessfully logged in, follow the next steps to create all required accounts and to create a credential file: # Move to AzureClusterlessHPC directory cd ~/.julia/dev/AzureClusterlessHPC # Install JSON parsing sudo apt-get update -y & sudo apt-get install -y jq # Create batch and storage accounts with given base name and region ./deploy.sh myname southcentralus The shell script writes the credentials to /path/to/AzureClusterlessHPC/credentials.json . Make sure to never check this credential file into git and keep it private! Test setup Start a Julia session and set the CREDENTIALS environment variable so that it points to your credential file: # Set path to credential file using Pkg ENV[\"CREDENTIALS\"] = Pkg.dir(\"AzureClusterlessHPC\", \"credentials.json\") # Load package using AzureClusterlessHPC If you were able to load the package without any warnings or erros, you're good to go. Proceed to the Quickstart section in the documentation or browse through the example notebooks .","title":"Installation"},{"location":"installation/#installation-and-setup","text":"Using AzureClusterlessHPC requires at the minimum one Azure Batch and one Azure Blob Storage account. Before being able to run the example, follow the instructions from this section to install the package and dependencies, and set up the necessary Azure resources.","title":"Installation and setup"},{"location":"installation/#prerequisites","text":"Ubuntu 18.04/20.04 or Debian 10 Julia v1.5 or later Python3 and pip3 Important : Your local Julia version needs to match the Julia version that is installed on the batch workers. The examples in this repository are set up for Julia 1.6.1.","title":"Prerequisites"},{"location":"installation/#install-julia-package","text":"Run the following command from an interactive Julia session to install the developer version of AzureClusterlessHPC.jl (press the ] key and then type the command) ] dev AzureClusterlessHPC.jl","title":"Install Julia package"},{"location":"installation/#install-python-dependencies-only-for-custom-julia-python","text":"If your Julia PyCall package uses the default Python version, all Python dependencies will be installed automatically. No actions are required. If you are using a custom Python version for PyCall, the Python dependencies need to be install manually into the Python environment that PyCall is using. E.g.: # Go to AzureClusterlessHPC directory cd ~/.julia/dev/AzureClusterlessHPC pip3 install -r pyrequirements.txt","title":"Install Python dependencies (only for custom Julia Python)"},{"location":"installation/#create-azure-storage-and-batch-account-with-aad-authentication","text":"Using AzureClusterlessHPC.jl requires an Azure Storage and Azure Batch account with AAD authentication. First, install the Azure Command Line Interface (CLI) by running (see here for additional instructions): curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash Next, log into your Azure account: az login Once you have sucessfully logged in, follow the next steps to create all required accounts and to create a credential file: # Move to AzureClusterlessHPC directory cd ~/.julia/dev/AzureClusterlessHPC # Install JSON parsing sudo apt-get update -y & sudo apt-get install -y jq # Create batch and storage accounts with given base name and region ./deploy.sh myname southcentralus The shell script writes the credentials to /path/to/AzureClusterlessHPC/credentials.json . Make sure to never check this credential file into git and keep it private!","title":"Create Azure Storage and Batch account with AAD authentication"},{"location":"installation/#test-setup","text":"Start a Julia session and set the CREDENTIALS environment variable so that it points to your credential file: # Set path to credential file using Pkg ENV[\"CREDENTIALS\"] = Pkg.dir(\"AzureClusterlessHPC\", \"credentials.json\") # Load package using AzureClusterlessHPC If you were able to load the package without any warnings or erros, you're good to go. Proceed to the Quickstart section in the documentation or browse through the example notebooks .","title":"Test setup"},{"location":"output/","text":"Collect output Fetch output Executing a function as a batch job via @batchexec returns a batch controller of type BatchController : # Test function @batchdef function hello_world(n) A = zeros(n, n) B = ones(n, n) return A, B end # Execute function as a multi-task batch job n = 2 batch_controller = @batchexec pmap(() -> hello_world(n), 1:2) # 2 tasks The batch controller has a field called batch_controller.output , which is a cell array of blob futures. The blob futures contain a (randomly generated) blob name of the future result stored in blob storage. E.g.: julia> batch_controller.output 2-element Array{Any,1}: BlobFuture(\"redwoodtemp\", BlobRef((\"o9UspZStMmqn\", \"TwIMfLrYiac2\"))) BlobFuture(\"redwoodtemp\", BlobRef((\"PxgtEgZonWPJ\", \"kZz1Wuknnag0\"))) The cell array contains one entry per task, i.e. length(batch_controller.output) is equal to the number of tasks of the executed batch job (in this case 2). As our function returns two arguments, each BlobRef contains two (future) blob names. To fetch the output of an executed function, AzureClusterlessHPC provides the fetch and fetch! functions. These functions can be either called on the batch controller output = fetch(batch_controller) or they can be directly called on the blob futures: # fetch called on batch controller output_job = fetch(batch_controller) # fetch called on blob future output_task_1 = fetch(batch_controller.output[1]) However, we recommend to always call fetch on the batch controller and not on the batch futures in .output . Calling fetch(batch_controller) is a blocking operation and waits for all batch tasks to terminate. Calling fetch(batch_controller.output[1]) is non-blocking and throws an exception if the task or job has not yet finished and the output is not yet available in blob storage. AzureClusterlessHPC also supplies in-place fetch functions, which not only return the output, but they also overwrite the BlobRef of the BlobFuture in batch_controller.output : # Inplace fetch output = fetch!(batch_controller) 2-element Array{Any,1}: ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0]) ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0]) batch_controller.output 2-element Array{Any,1}: BlobFuture(\"redwoodtemp\", ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0])) BlobFuture(\"redwoodtemp\", ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0])) Inplace fetch! by default deletes the referenced blob objects. If fetch! is called on the batch controller again, it will then throw an error. To avoid deleting the blob, call fetch!(batch_controller; destroy_blob=false) . Fetch and reduce output AzureClusterlessHPC supplies the fetchreduce and fetchreduce! functions to collect the output from multiple tasks and apply a specified reduction operation to the output. E.g. using the prior example: # Test function @batchdef function hello_world(n) A = ones(n, n) B = 2 .* ones(n, n) return A, B end # Execute function as a multi-task batch job n = 2 batch_controller = @batchexec pmap(() -> hello_world(n), 1:2) # 2 tasks We can fetch and sum the output via: output_sum = fetchreduce(batch_controller; op=+, remote=false) # Returns ([2.0 2.0; 2.0 2.0], [4.0 4.0; 4.0 4.0]) The remote keyword argument specifies where the summation is execute. By default, the output is collected and summed on the master. For remote=true , AzureClusterlessHPC will schedule the summation tasks on idle instances in the batch pool and only the final (reduced) argument is copied back to the master. We can also initialize the output ourselves and then call the in-place fetchreduce! function: # Initialize output output = (zeros(2, 2), zeros(2, 2)) # Fetch output and sum fetchreduce!(batch_controller, output; op=+) @show output output = ([2.0 2.0; 2.0 2.0], [4.0 4.0; 4.0 4.0]) Fetch output and retry failed tasks To fetch the output of one or multiple tasks and allow that failed tasks are restarted, pass the num_restart keyword argument to the fetch / fetch! or fetchreduce / fetchreduce! functions: # Fetch output and allow two retrys per task in case of failures output_job = fetch(batch_controller; num_restart=2)","title":"Collect output"},{"location":"output/#collect-output","text":"","title":"Collect output"},{"location":"output/#fetch-output","text":"Executing a function as a batch job via @batchexec returns a batch controller of type BatchController : # Test function @batchdef function hello_world(n) A = zeros(n, n) B = ones(n, n) return A, B end # Execute function as a multi-task batch job n = 2 batch_controller = @batchexec pmap(() -> hello_world(n), 1:2) # 2 tasks The batch controller has a field called batch_controller.output , which is a cell array of blob futures. The blob futures contain a (randomly generated) blob name of the future result stored in blob storage. E.g.: julia> batch_controller.output 2-element Array{Any,1}: BlobFuture(\"redwoodtemp\", BlobRef((\"o9UspZStMmqn\", \"TwIMfLrYiac2\"))) BlobFuture(\"redwoodtemp\", BlobRef((\"PxgtEgZonWPJ\", \"kZz1Wuknnag0\"))) The cell array contains one entry per task, i.e. length(batch_controller.output) is equal to the number of tasks of the executed batch job (in this case 2). As our function returns two arguments, each BlobRef contains two (future) blob names. To fetch the output of an executed function, AzureClusterlessHPC provides the fetch and fetch! functions. These functions can be either called on the batch controller output = fetch(batch_controller) or they can be directly called on the blob futures: # fetch called on batch controller output_job = fetch(batch_controller) # fetch called on blob future output_task_1 = fetch(batch_controller.output[1]) However, we recommend to always call fetch on the batch controller and not on the batch futures in .output . Calling fetch(batch_controller) is a blocking operation and waits for all batch tasks to terminate. Calling fetch(batch_controller.output[1]) is non-blocking and throws an exception if the task or job has not yet finished and the output is not yet available in blob storage. AzureClusterlessHPC also supplies in-place fetch functions, which not only return the output, but they also overwrite the BlobRef of the BlobFuture in batch_controller.output : # Inplace fetch output = fetch!(batch_controller) 2-element Array{Any,1}: ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0]) ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0]) batch_controller.output 2-element Array{Any,1}: BlobFuture(\"redwoodtemp\", ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0])) BlobFuture(\"redwoodtemp\", ([0.0 0.0; 0.0 0.0], [1.0 1.0; 1.0 1.0])) Inplace fetch! by default deletes the referenced blob objects. If fetch! is called on the batch controller again, it will then throw an error. To avoid deleting the blob, call fetch!(batch_controller; destroy_blob=false) .","title":"Fetch output"},{"location":"output/#fetch-and-reduce-output","text":"AzureClusterlessHPC supplies the fetchreduce and fetchreduce! functions to collect the output from multiple tasks and apply a specified reduction operation to the output. E.g. using the prior example: # Test function @batchdef function hello_world(n) A = ones(n, n) B = 2 .* ones(n, n) return A, B end # Execute function as a multi-task batch job n = 2 batch_controller = @batchexec pmap(() -> hello_world(n), 1:2) # 2 tasks We can fetch and sum the output via: output_sum = fetchreduce(batch_controller; op=+, remote=false) # Returns ([2.0 2.0; 2.0 2.0], [4.0 4.0; 4.0 4.0]) The remote keyword argument specifies where the summation is execute. By default, the output is collected and summed on the master. For remote=true , AzureClusterlessHPC will schedule the summation tasks on idle instances in the batch pool and only the final (reduced) argument is copied back to the master. We can also initialize the output ourselves and then call the in-place fetchreduce! function: # Initialize output output = (zeros(2, 2), zeros(2, 2)) # Fetch output and sum fetchreduce!(batch_controller, output; op=+) @show output output = ([2.0 2.0; 2.0 2.0], [4.0 4.0; 4.0 4.0])","title":"Fetch and reduce output"},{"location":"output/#fetch-output-and-retry-failed-tasks","text":"To fetch the output of one or multiple tasks and allow that failed tasks are restarted, pass the num_restart keyword argument to the fetch / fetch! or fetchreduce / fetchreduce! functions: # Fetch output and allow two retrys per task in case of failures output_job = fetch(batch_controller; num_restart=2)","title":"Fetch output and retry failed tasks"},{"location":"pool/","text":"Managing pools Start a pool To start a batch pool and (optionally) install a set of specified Julia packages on the workers, we first need to create a bash script of the following form, which will be executed by each node joining the pool: #!/bin/bash ################################################################################################### # DO NOT MODIFY! # Switch to superuser and load module sudo bash pwd # Install Julia wget \"https://julialang-s3.julialang.org/bin/linux/x64/1.5/julia-1.5.2-linux-x86_64.tar.gz\" tar -xvzf julia-1.5.2-linux-x86_64.tar.gz rm -rf julia-1.5.2-linux-x86_64.tar.gz ln -s /mnt/batch/tasks/startup/wd/julia-1.5.2/bin/julia /usr/local/bin/julia # Install AzureClusterlessHPC git clone https://github.com/microsoft/AzureClusterlessHPC.jl julia -e 'using Pkg; Pkg.add(url=joinpath(pwd(), \"AzureClusterlessHPC\"))' ################################################################################################### # ADD USER PACKAGES HERE # ... ################################################################################################### # DO NOT MODIFY! # Make julia dir available for all users chmod -R 777 /mnt/batch/tasks/startup/wd/.julia If you need to install Julia packages for your application, specify the packages in the section # ADD USER PACKAGES HERE . E.g. to install the Julia package IterativeSolvers.jl , add the line: julia -e 'using Pkg; Pkg.add(\"IterativeSolvers\")' To install packages that are not officially registered with Julia, use this line to add packages: julia -e 'using Pkg; Pkg.develop(PackageSpec(url=\"https://github.com/slimgroup/JOLI.jl\"))' Save this batch script, e.g. as pool_startup_script.sh . You can now create a pool in which the startup script will be executed on each node that joins the pool: # Path to bash file startup_script = \"/path/to/pool_startup_script.sh\" # Create pool create_pool_and_resource_file(startup_script; enable_auto_scale=false, auto_scale_formula=nothing, auto_scale_evaluation_interval_minutes=nothing, image_resource_id=nothing) Once you run the pool creation command, an active pool is added to the current session. You can list the currently active pools with AzureClusterlessHPC.__active_pools__ . Important : If you start a new session, you need to run the create_pool_and_resource_file again, even if your pool still exists, so that the pool is added to the active pools of the session. Required input arguments: startup_script : String that defines the path and name of the bash startup script. Optional keyword arguments : enable_auto_scale=false : Enable auto scaling. If true , the keyword arguments auto_scale_formula and auto_scale_evaluation_interval_minutes must be provided as well. If the parameter _POOL_NODE_COUNT has been set, it will be ignored. auto_scale_formula=nothing : String that defines the auto-scaling behavior. See here for Azure Batch auto-scaling templates. auto_scale_evaluation_interval_minutes=nothing : Time interval between evaluations of the auto-scaling function. The minimum possible interval is 5 minutes. image_resource_id=nothing : Provide an optional image resource ID to use a custom machine image for nodes joining the batch pool. Pools with managed VM images To launch a pool with a custom VM image, you need to create a custom VM image and then upload it to the Azure shared image gallery. The image gallery will assign an image reference ID to the image (see here for details on how to create a shared image). Once you have the shared image ID, pass it as a keyword argument image_resource_id to the create_pool function. If you do not pass the image ID to the function, workers are created with the default Ubuntu image, which does not have Julia installed. # Image resource ID image_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # Create pool with custom VM image create_pool(image_resource_id=image_id, enable_auto_scale=false, auto_scale_formula=nothing, auto_scale_evaluation_interval_minutes=nothing) Optional keyword arguments: image_resource_id=nothing : Image resource ID to use a custom machine image for nodes joining the batch pool. For a description of all other keyword arguments, see the above section. Important : In your parameter file, set the variable \"_JULIA_DEPOT_PATH\" to the path where Julia is installed on the image. Pools with Docker As a third alternative, you can create an application package using Docker. You first create or specify a Docker image, which will then be pre-installed on each VM joining the batch pool. See the example directory /path/to/redwood/examples/container for an example Dockerfile. Follow the subsequent instructions to create a Docker image from a Dockerfile and upload it to your (personal) container repository: # Move to directory with Dockerfile cd /path/to/redwood/examples/container # Build image docker build -t redoowd:v1.0 . # Login docker login # Tag and push docker tag redwood:v1.0 username/redwood:v1.0 docker push username/redwood:v1.0 Once you have a Docker image in a public repository, you can specify a Docker image in your parameters.json file: \"_CONTAINER\": \"username/redwood:v1.0\" If the _CONTAINER parameter is set, AzureClusterlessHPC will install the specified container image on the VMs in the batch pool. Pool autoscaling To create a pool with auto-scaling, use one of the above commands and set the following keyword arguments: Set the keyword argument enable_auto_scale=true Define an auto-scaling formula. E.g. the following formula creates a pool with 1 node and resizes the pool to up to 10 VMs based on the number of pending tasks: auto_scale_formula = \"\"\"startingNumberOfVMs = 1; maxNumberofVMs = 10; pendingTaskSamplePercent = \\$PendingTasks.GetSamplePercent(30 * TimeInterval_Second); pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg(\\$PendingTasks.GetSample(30 * TimeInterval_Second)); \\$TargetDedicatedNodes=min(maxNumberofVMs, pendingTaskSamples); \\$NodeDeallocationOption = taskcompletion;\"\"\" For other auto-scaling formulas, refer to the Azure Batch documentation . Set the auto-scaling interval: auto_scale_evaluation_interval_minutes=5 . The minimum allowed values is 5 minutes. The full example would look like this: # Pool startup script startup_script = \"/path/to/pool_startup_script.sh\" # Autoscale formula auto_scale_formula = \"\"\"startingNumberOfVMs = 1; maxNumberofVMs = 10; pendingTaskSamplePercent = \\$PendingTasks.GetSamplePercent(30 * TimeInterval_Second); pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg(\\$PendingTasks.GetSample(30 * TimeInterval_Second)); \\$TargetDedicatedNodes=min(maxNumberofVMs, pendingTaskSamples); \\$NodeDeallocationOption = taskcompletion;\"\"\" create_pool_and_resource_file(startup_script; enable_auto_scale=true, auto_scale_formula=auto_scale_formula, auto_scale_evaluation_interval_minutes=5) Pool resize Currently not supported.","title":"Manage pools"},{"location":"pool/#managing-pools","text":"","title":"Managing pools"},{"location":"pool/#start-a-pool","text":"To start a batch pool and (optionally) install a set of specified Julia packages on the workers, we first need to create a bash script of the following form, which will be executed by each node joining the pool: #!/bin/bash ################################################################################################### # DO NOT MODIFY! # Switch to superuser and load module sudo bash pwd # Install Julia wget \"https://julialang-s3.julialang.org/bin/linux/x64/1.5/julia-1.5.2-linux-x86_64.tar.gz\" tar -xvzf julia-1.5.2-linux-x86_64.tar.gz rm -rf julia-1.5.2-linux-x86_64.tar.gz ln -s /mnt/batch/tasks/startup/wd/julia-1.5.2/bin/julia /usr/local/bin/julia # Install AzureClusterlessHPC git clone https://github.com/microsoft/AzureClusterlessHPC.jl julia -e 'using Pkg; Pkg.add(url=joinpath(pwd(), \"AzureClusterlessHPC\"))' ################################################################################################### # ADD USER PACKAGES HERE # ... ################################################################################################### # DO NOT MODIFY! # Make julia dir available for all users chmod -R 777 /mnt/batch/tasks/startup/wd/.julia If you need to install Julia packages for your application, specify the packages in the section # ADD USER PACKAGES HERE . E.g. to install the Julia package IterativeSolvers.jl , add the line: julia -e 'using Pkg; Pkg.add(\"IterativeSolvers\")' To install packages that are not officially registered with Julia, use this line to add packages: julia -e 'using Pkg; Pkg.develop(PackageSpec(url=\"https://github.com/slimgroup/JOLI.jl\"))' Save this batch script, e.g. as pool_startup_script.sh . You can now create a pool in which the startup script will be executed on each node that joins the pool: # Path to bash file startup_script = \"/path/to/pool_startup_script.sh\" # Create pool create_pool_and_resource_file(startup_script; enable_auto_scale=false, auto_scale_formula=nothing, auto_scale_evaluation_interval_minutes=nothing, image_resource_id=nothing) Once you run the pool creation command, an active pool is added to the current session. You can list the currently active pools with AzureClusterlessHPC.__active_pools__ . Important : If you start a new session, you need to run the create_pool_and_resource_file again, even if your pool still exists, so that the pool is added to the active pools of the session. Required input arguments: startup_script : String that defines the path and name of the bash startup script. Optional keyword arguments : enable_auto_scale=false : Enable auto scaling. If true , the keyword arguments auto_scale_formula and auto_scale_evaluation_interval_minutes must be provided as well. If the parameter _POOL_NODE_COUNT has been set, it will be ignored. auto_scale_formula=nothing : String that defines the auto-scaling behavior. See here for Azure Batch auto-scaling templates. auto_scale_evaluation_interval_minutes=nothing : Time interval between evaluations of the auto-scaling function. The minimum possible interval is 5 minutes. image_resource_id=nothing : Provide an optional image resource ID to use a custom machine image for nodes joining the batch pool.","title":"Start a pool"},{"location":"pool/#pools-with-managed-vm-images","text":"To launch a pool with a custom VM image, you need to create a custom VM image and then upload it to the Azure shared image gallery. The image gallery will assign an image reference ID to the image (see here for details on how to create a shared image). Once you have the shared image ID, pass it as a keyword argument image_resource_id to the create_pool function. If you do not pass the image ID to the function, workers are created with the default Ubuntu image, which does not have Julia installed. # Image resource ID image_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # Create pool with custom VM image create_pool(image_resource_id=image_id, enable_auto_scale=false, auto_scale_formula=nothing, auto_scale_evaluation_interval_minutes=nothing) Optional keyword arguments: image_resource_id=nothing : Image resource ID to use a custom machine image for nodes joining the batch pool. For a description of all other keyword arguments, see the above section. Important : In your parameter file, set the variable \"_JULIA_DEPOT_PATH\" to the path where Julia is installed on the image.","title":"Pools with managed VM images"},{"location":"pool/#pools-with-docker","text":"As a third alternative, you can create an application package using Docker. You first create or specify a Docker image, which will then be pre-installed on each VM joining the batch pool. See the example directory /path/to/redwood/examples/container for an example Dockerfile. Follow the subsequent instructions to create a Docker image from a Dockerfile and upload it to your (personal) container repository: # Move to directory with Dockerfile cd /path/to/redwood/examples/container # Build image docker build -t redoowd:v1.0 . # Login docker login # Tag and push docker tag redwood:v1.0 username/redwood:v1.0 docker push username/redwood:v1.0 Once you have a Docker image in a public repository, you can specify a Docker image in your parameters.json file: \"_CONTAINER\": \"username/redwood:v1.0\" If the _CONTAINER parameter is set, AzureClusterlessHPC will install the specified container image on the VMs in the batch pool.","title":"Pools with Docker"},{"location":"pool/#pool-autoscaling","text":"To create a pool with auto-scaling, use one of the above commands and set the following keyword arguments: Set the keyword argument enable_auto_scale=true Define an auto-scaling formula. E.g. the following formula creates a pool with 1 node and resizes the pool to up to 10 VMs based on the number of pending tasks: auto_scale_formula = \"\"\"startingNumberOfVMs = 1; maxNumberofVMs = 10; pendingTaskSamplePercent = \\$PendingTasks.GetSamplePercent(30 * TimeInterval_Second); pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg(\\$PendingTasks.GetSample(30 * TimeInterval_Second)); \\$TargetDedicatedNodes=min(maxNumberofVMs, pendingTaskSamples); \\$NodeDeallocationOption = taskcompletion;\"\"\" For other auto-scaling formulas, refer to the Azure Batch documentation . Set the auto-scaling interval: auto_scale_evaluation_interval_minutes=5 . The minimum allowed values is 5 minutes. The full example would look like this: # Pool startup script startup_script = \"/path/to/pool_startup_script.sh\" # Autoscale formula auto_scale_formula = \"\"\"startingNumberOfVMs = 1; maxNumberofVMs = 10; pendingTaskSamplePercent = \\$PendingTasks.GetSamplePercent(30 * TimeInterval_Second); pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg(\\$PendingTasks.GetSample(30 * TimeInterval_Second)); \\$TargetDedicatedNodes=min(maxNumberofVMs, pendingTaskSamples); \\$NodeDeallocationOption = taskcompletion;\"\"\" create_pool_and_resource_file(startup_script; enable_auto_scale=true, auto_scale_formula=auto_scale_formula, auto_scale_evaluation_interval_minutes=5)","title":"Pool autoscaling"},{"location":"pool/#pool-resize","text":"Currently not supported.","title":"Pool resize"},{"location":"quickstart/","text":"Quick start To run this example, you need to have followed the installation and setup steps in Installation . Complete the steps in the installation section before proceeding. See section Credentials for additional information on resources and credentials. To run our first example, we move to the /examples/batch directory and have a look at the directory content: # Go to example directory cd ~/.julia/dev/AzureClusterlessHPC/examples/batch # List directory content ls -l julia_batch_macros.ipynb parameters.json pool_startup_script.sh We can see that our directory contains a parameters.json file with job parameters such as the number and type of Azure instances for our pool. (See section User parameters for a list of all available job parameters.) Additionally, we have a pool_startup_script.sh that is executed by every new VM joining a pool and which allows us to specify dependiencies that need to be installed on the worker nodes. Alternatively, we can also start pools with a managed VM image or using Docker images . Next, we start a Julia session and set the environment variables CREDENTIALS and PARAMETERS so that they point to your parameter and credential file. You can either set the variables in your bash terminal (e.g. in your ~/.bashrc file), or directly in the Julia terminal: # Set path to credentials in Julia ENV[\"CREDENTIALS\"] = joinpath(pwd(), \"../..\", \"credentials.json\") # Set path to batch parameters (pool id, VM types, etc.) ENV[\"PARAMETERS\"] = joinpath(pwd(), \"parameters.json\") Next, load AzureClusterlessHPC.jl and create a pool with the parameters from parameters.json and using our pool_startup_script.sh : # Load package using AzureClusterlessHPC # Create default pool with parameters from parameters.json startup_script = \"pool_startup_script.sh\" create_pool_and_resource_file(startup_script) You can check the status of your batch pool in the Microsoft Azure Portal or with the Azure Batch Explorer (recommended). Now you can execute Julia functions that are defined using the @batchdef macro via Azure batch: # Define function @batchdef function hello_world(name) print(\"Hello $name\") return \"Goodbye\" end # Execute function via Azure batch @batchexec hello_world(\"Bob\") You can also run multi-tasks batch job using the pmap function in combination with @batchdef : # Run a multi-task batch job @batchexec pmap(name -> hello_world(name), [\"Bob\", \"Jane\"]) To delete all resources run: # Shut down pool delete_pool() # Delete container with temporary blob files delete_container()","title":"Quickstart"},{"location":"quickstart/#quick-start","text":"To run this example, you need to have followed the installation and setup steps in Installation . Complete the steps in the installation section before proceeding. See section Credentials for additional information on resources and credentials. To run our first example, we move to the /examples/batch directory and have a look at the directory content: # Go to example directory cd ~/.julia/dev/AzureClusterlessHPC/examples/batch # List directory content ls -l julia_batch_macros.ipynb parameters.json pool_startup_script.sh We can see that our directory contains a parameters.json file with job parameters such as the number and type of Azure instances for our pool. (See section User parameters for a list of all available job parameters.) Additionally, we have a pool_startup_script.sh that is executed by every new VM joining a pool and which allows us to specify dependiencies that need to be installed on the worker nodes. Alternatively, we can also start pools with a managed VM image or using Docker images . Next, we start a Julia session and set the environment variables CREDENTIALS and PARAMETERS so that they point to your parameter and credential file. You can either set the variables in your bash terminal (e.g. in your ~/.bashrc file), or directly in the Julia terminal: # Set path to credentials in Julia ENV[\"CREDENTIALS\"] = joinpath(pwd(), \"../..\", \"credentials.json\") # Set path to batch parameters (pool id, VM types, etc.) ENV[\"PARAMETERS\"] = joinpath(pwd(), \"parameters.json\") Next, load AzureClusterlessHPC.jl and create a pool with the parameters from parameters.json and using our pool_startup_script.sh : # Load package using AzureClusterlessHPC # Create default pool with parameters from parameters.json startup_script = \"pool_startup_script.sh\" create_pool_and_resource_file(startup_script) You can check the status of your batch pool in the Microsoft Azure Portal or with the Azure Batch Explorer (recommended). Now you can execute Julia functions that are defined using the @batchdef macro via Azure batch: # Define function @batchdef function hello_world(name) print(\"Hello $name\") return \"Goodbye\" end # Execute function via Azure batch @batchexec hello_world(\"Bob\") You can also run multi-tasks batch job using the pmap function in combination with @batchdef : # Run a multi-task batch job @batchexec pmap(name -> hello_world(name), [\"Bob\", \"Jane\"]) To delete all resources run: # Shut down pool delete_pool() # Delete container with temporary blob files delete_container()","title":"Quick start"}]}